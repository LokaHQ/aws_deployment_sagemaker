{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS SageMaker Inference Immersion Day\n",
    "This notebook shows how to:\n",
    "* __Lab 1:__ Deploy a real time endpoint with a prebuilt container and invoke it.\n",
    "* __Lab 2:__ Deploy a real time endpoint with a custom container.\n",
    "* __Lab 3:__ Host an endpoint with multiple production variants with different traffic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Setup (~20 - 30 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Account Configuration with Event Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please follow the instructions to configure the AWS account that you will be using for this workshop. Go to https://dashboard.eventengine.run/ and paste the `Event hash` provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Studio Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a SageMaker Studio User clicking on `Add user`. Once it's been created, click on the user name and copy the IAM role associated with this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![config](../assets/sagemaker-studio-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![config](../assets/sagemaker-studio-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to IAM and search for the role and click on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![config](../assets/sagemaker-studio-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on `Trust Relationships` and in the Edit Trust Relationship text area, paste the following JSON, then click on `Update Trust Policy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![config](../assets/sagemaker-studio-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "              \"Service\": \"sagemaker.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "              \"Service\": \"codebuild.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "         }\n",
    "    ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Trust Relationships` tab should show 2 Trusted entities now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![config](../assets/sagemaker-studio-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to run docker image building from Studio notebook, you need to add an inline policy to this role as follows. Frist click on `Permissions` tab, then click on `Add inline policy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![config](../assets/sagemaker-studio-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `Create policy` page, click on `JSON` tab then copy the following text and paste it in the text area replacing the default text, then click on `Review Policy` follow the remaining steps to create the inline policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![config](../assets/sagemaker-studio-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"codebuild:DeleteProject\",\n",
    "                \"codebuild:CreateProject\",\n",
    "                \"codebuild:BatchGetBuilds\",\n",
    "                \"codebuild:StartBuild\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:codebuild:*:*:project/sagemaker-studio*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"logs:CreateLogStream\",\n",
    "            \"Resource\": \"arn:aws:logs:*:*:log-group:/aws/codebuild/sagemaker-studio*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"logs:GetLogEvents\",\n",
    "                \"logs:PutLogEvents\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:logs:*:*:log-group:/aws/codebuild/sagemaker-studio*:log-stream:*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"logs:CreateLogGroup\",\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"ecr:CreateRepository\",\n",
    "                \"ecr:BatchGetImage\",\n",
    "                \"ecr:CompleteLayerUpload\",\n",
    "                \"ecr:DescribeImages\",\n",
    "                \"ecr:DescribeRepositories\",\n",
    "                \"ecr:UploadLayerPart\",\n",
    "                \"ecr:ListImages\",\n",
    "                \"ecr:InitiateLayerUpload\",\n",
    "                \"ecr:BatchCheckLayerAvailability\",\n",
    "                \"ecr:PutImage\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:ecr:*:*:repository/sagemaker-studio*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"ecr:GetAuthorizationToken\",\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:DeleteObject\",\n",
    "                \"s3:PutObject\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:s3:::sagemaker-*/*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:CreateBucket\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:s3:::sagemaker*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"iam:GetRole\",\n",
    "                \"iam:ListRoles\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"iam:PassRole\",\n",
    "            \"Resource\": \"arn:aws:iam::*:role/*\",\n",
    "            \"Condition\": {\n",
    "                \"StringLikeIfExists\": {\n",
    "                    \"iam:PassedToService\": \"codebuild.amazonaws.com\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Import APIs to be used by the notebook. For almost all of the tasks presented in this notebook, we'll be using [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from sagemaker.session import production_variant\n",
    "from sagemaker import get_execution_role, image_uris, Session\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.clarify import (\n",
    "    BiasConfig,\n",
    "    DataConfig,\n",
    "    ModelConfig,\n",
    "    ModelPredictedLabelConfig,\n",
    "    SHAPConfig,\n",
    ")\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.model_monitor import (\n",
    "    BiasAnalysisConfig,\n",
    "    CronExpressionGenerator,\n",
    "    DataCaptureConfig,\n",
    "    EndpointInput,\n",
    "    ExplainabilityAnalysisConfig,\n",
    "    ModelBiasMonitor,\n",
    "    ModelExplainabilityMonitor,\n",
    ")\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handful of configuration\n",
    "Here, we are configuring the execution role that we'll be using for deploying everything, the bucket where we'll save artifacts and data and some prefixes to save data in separate folders according to our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "print(f\"RoleArn: {role}\")\n",
    "\n",
    "boto_session = boto3.session.Session()\n",
    "\n",
    "sagemaker_session = Session(boto_session)\n",
    "sagemaker_client = sagemaker_session.sagemaker_client\n",
    "sagemaker_runtime_client = sagemaker_session.sagemaker_runtime_client\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "print(f\"AWS region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A different bucket can be used, but make sure the role for this notebook has\n",
    "# the s3:PutObject permissions. This is the bucket into which the data is captured\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "print(f\"Demo Bucket: {bucket}\")\n",
    "prefix = \"sagemaker/DEMO-ClarifyModelMonitor-20200901\"\n",
    "s3_key = f\"s3://{bucket}/{prefix}\"\n",
    "print(f\"S3 key: {s3_key}\")\n",
    "\n",
    "s3_capture_upload_path = f\"{s3_key}/datacapture\"\n",
    "ground_truth_upload_path = f\"{s3_key}/ground_truth_data/{datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    "s3_report_path = f\"{s3_key}/reports\"\n",
    "\n",
    "print(f\"Capture path: {s3_capture_upload_path}\")\n",
    "print(f\"Ground truth path: {ground_truth_upload_path}\")\n",
    "print(f\"Report path: {s3_report_path}\")\n",
    "\n",
    "baseline_results_uri = f\"{s3_key}/baselining\"\n",
    "print(f\"Baseline results uri: {baseline_results_uri}\")\n",
    "\n",
    "endpoint_instance_count = 1\n",
    "endpoint_instance_type = \"ml.m5.large\"\n",
    "schedule_expression = CronExpressionGenerator.hourly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model files and data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"model/xgb-churn-prediction-model.tar.gz\"\n",
    "test_file = \"test_data/test-file.txt\"\n",
    "test_dataset = \"test_data/test.csv\"\n",
    "validation_dataset = \"test_data/validation-dataset-with-header.csv\"\n",
    "dataset_type = \"text/csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(validation_dataset) as f:\n",
    "    headers_line = f.readline().rstrip()\n",
    "all_headers = headers_line.split(\",\")\n",
    "label_header = all_headers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that the execution role for this notebook has the necessary permissions to proceed. Put a simple test object into the S3 bucket speciﬁed above. If this command fails, update the role to have `s3:PutObject` permission on the bucket and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload a test file\n",
    "S3Uploader.upload(test_file, f\"s3://{bucket}/test_upload\", sagemaker_session=sagemaker_session)\n",
    "print(\"Success! We are all set to proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 1: Deploying a real-time endpoint on Amazon SageMaker (~20-25 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we trained a model beforehand for you, so we'll be using the resulting artifact here to save some time. If you want to know how this model was trained, please refer to the [training notebook](../training/xgboost_customer_churn.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the pre-trained model to Amazon S3\n",
    "As an example, this code uploads a pre-trained XGBoost model that is ready for to be deployed. This model was trained using the [code that you can find on training folder](../training/xgboost_customer_churn.ipynb) in SageMaker. In order to deploy an endpoint, we will need to first upload the model artifact (the serialized object) to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = S3Uploader.upload(local_path=model_file, desired_s3_uri=s3_key, sagemaker_session=sagemaker_session)\n",
    "print(f\"Model file has been uploaded to {model_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model with Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with deploying a pre-trained churn prediction model. Here, create the SageMaker `Model` object with the image and model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = f\"DEMO-xgb-churn-pred-model-monitor-{datetime.utcnow():%Y-%m-%d-%H%M%S}\"\n",
    "print(\"Model name: \", model_name)\n",
    "endpoint_name = f\"DEMO-xgb-churn-model-monitor-{datetime.utcnow():%Y-%m-%d-%H%M%S}\"\n",
    "print(\"Endpoint name: \", endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a trained model, you can include it in a Docker container that runs your inference code. A container provides an effectively isolated environment, ensuring a consistent runtime regardless of where the container is deployed. Containerizing your model and code enables fast and reliable deployment of your model.\n",
    "\n",
    "We are going to use an already pre-built docker image with `xgboost 0.90-1` version by SageMaker. In order to do this, we will retrieve the ECR docker image URL from Amazon ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = image_uris.retrieve(\"xgboost\", region, \"0.90-1\")\n",
    "print(f\"XGBoost image uri: {image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker already has multiple pre-built docker images for you to use or extend. For more info on this please refer to these links:\n",
    "\n",
    "- [Deep Learning Docker Images](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html)\n",
    "- [Sklearn and Spark ML](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-docker-containers-scikit-learn-spark.html)\n",
    "\n",
    "Also, Sagemaker Python SDK has already a high-level interface called `Estimator` to handle end-to-end training and deployment of most common ML and Deep Learning frameworks that you can find out there. Also, if __you already have a model that you trained somewhere else__, you can use `Model` interface to deploy a model as an endpoint in SageMaker. Refer to this links if you want to go deeper on your framework of interest.\n",
    "\n",
    "- [Scikit-learn](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/index.html)\n",
    "- [SparkML](https://sagemaker.readthedocs.io/en/stable/frameworks/sparkml/index.html)\n",
    "- [Tensorflow](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/index.html)\n",
    "- [PyTorch](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/index.html)\n",
    "- [HuggingFace](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/index.html)\n",
    "- [MXNet](https://sagemaker.readthedocs.io/en/stable/frameworks/mxnet/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the model, we need to pass the mentioned Docker ECR uri, an IAM role used to access the data on S3 and create the endpoint on SageMaker, the model url on S3. Also in order to deploy an endpoint, we will to configure the instance (count and type) and a serializer that will define how the data will be encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    # CODE STARTS HERE\n",
    "\n",
    "    # CODE ENDS HERE\n",
    ")\n",
    "print(f\"Deploying model {model_name} to endpoint {endpoint_name}\")\n",
    "model.deploy(\n",
    "    # CODE STARTS HERE\n",
    "    \n",
    "    # CODE ENDS HERE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are using the `SageMaker runtime client` to send some data to our realtime endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sending test traffic to the endpoint {endpoint_name}. \\nPlease wait\", end=\"\")\n",
    "test_dataset_size = 0  # record the number of rows in data we're sending for inference\n",
    "count = 0\n",
    "with open(test_dataset, \"r\") as f:\n",
    "    for row in f:\n",
    "        if test_dataset_size < 10:\n",
    "            payload = row.rstrip(\"\\n\")\n",
    "            response = sagemaker_runtime_client.invoke_endpoint(\n",
    "                # CODE STARTS HERE\n",
    "                # CODE ENDS HERE\n",
    "            )\n",
    "            prediction = response[\"Body\"].read()\n",
    "            print(prediction)\n",
    "            \n",
    "            time.sleep(0.5)\n",
    "        test_dataset_size += 1\n",
    "\n",
    "print()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions correspond here to the actual probability of a client to churn or not, which goes from 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 2: Deploy a model with a custom docker image (~15-20 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some cases where you want to use a custom docker image environment to deploy your models. For those, you can actually build your own customized docker image to deploy your model with SageMaker. If you want to know how this model was trained, please refer to the [training notebook](../training/scikitlearn_churn_prediction.py.ipynb).\n",
    "\n",
    "This will cover:\n",
    "- Building and pushing a docker image to Amazon Elastic Container Registry (Amazon ECR).\n",
    "- Using that image for deploying the model with SageMaker.\n",
    "- Invoking the endpoint using the SageMaker runtime client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The docker image that we will be using has the following files:\n",
    "\n",
    "- __Dockerfile__: Specification for building your docker image.\n",
    "- __nginx.conf__: Nginx configuration file.\n",
    "- __wsgi.py__: Wrapper for gunicorn.\n",
    "- __serve__: Entrypoint for sagemaker to start the gunicorn server and nginx proxy.\n",
    "- __sklearn_model.joblib__: Model artifact result of [training notebook](../training/scikitlearn_churn_prediction.py.ipynb).\n",
    "- __predictor.py__: Inference code, flask simple rest api with 2 endpoints, `/ping` and `/invocations`.\n",
    "- __requirements.txt__: Python requirements.\n",
    "- __build_and_push.sh__: Utility script for build and pushing your ECR image locally. This can used in replacement of `sm-docker`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This files can be found inside the [custom_container folder](custom_container/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might encounter another examples that use a most updated way for deploying your custom containers called __sagemaker inference toolkit__ which is the recommended. However, you can still use both without problems. For more info on this please refer to this [link](https://github.com/aws/sagemaker-inference-toolkit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and push your docker image to ECR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a docker image for packaging our new model trained with the same dataset the first model was trained on but with a different algorithm, this case implemented with scikit-learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will:\n",
    "- Create an ECR repository if does not exist.\n",
    "- Build a docker image and tag it accordingly\n",
    "- Push the docker image that has been built to the created ECR repo.\n",
    "\n",
    "In order to do this from SageMaker studio notebook we need to use `sm-docker`. If you want to replicate this running locally, please refer to [sagemaker-inference-immersion-day-local-ver.ipynb](local_notebook/sagemaker-inference-immersion-day-local-ver.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda update setuptools -y && pip install sagemaker-studio-image-build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cd custom_container && sm-docker build . --repository sagemaker-studio-sklearn-custom:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the endpoint using the custom image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: Before executing next cells, assign the image uri/repository that you got from last cell in the next one to the `image_uri_custom` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri_custom = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_custom = f\"DEMO-sklearn-churn-predictor-{datetime.utcnow():%Y-%m-%d-%H%M%S}\"\n",
    "print(\"Model name: \", model_name_custom)\n",
    "endpoint_name_custom = f\"DEMO-sklearn-churn-predictor-{datetime.utcnow():%Y-%m-%d-%H%M%S}\"\n",
    "print(\"Endpoint name: \", endpoint_name_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    role=role,\n",
    "    name=model_name_custom,\n",
    "    image_uri=image_uri_custom,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "print(f\"Deploying model {model_name_custom} to endpoint {endpoint_name_custom}\")\n",
    "model.deploy(\n",
    "    initial_instance_count=endpoint_instance_count,\n",
    "    instance_type=endpoint_instance_type,\n",
    "    endpoint_name=endpoint_name_custom\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did before, for invoking the endpoint, we use the `sagemaker runtime client`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sending test traffic to the endpoint {endpoint_name_custom}. \\nPlease wait\", end=\"\")\n",
    "test_dataset_size = 0  # record the number of rows in data we're sending for inference\n",
    "count = 0\n",
    "with open(test_dataset, \"r\") as f:\n",
    "    for row in f:\n",
    "        if test_dataset_size < 10:\n",
    "            payload = row.rstrip(\"\\n\")\n",
    "            response = sagemaker_runtime_client.invoke_endpoint(\n",
    "                EndpointName=endpoint_name_custom,\n",
    "                Body=payload[2:],\n",
    "                ContentType=dataset_type,\n",
    "            )\n",
    "            prediction = response[\"Body\"].read()\n",
    "            print(prediction)\n",
    "            \n",
    "            time.sleep(0.5)\n",
    "        test_dataset_size += 1\n",
    "\n",
    "print()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 3: Production Variants and A/B Testing (~15-20 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker enables you to test multiple models or model versions behind the same endpoint using production variants. Each production variant identifies a machine learning (ML) model and the resources deployed for hosting the model. You can distribute endpoint invocation requests across multiple production variants by providing the traffic distribution for each variant, or you can invoke a specific variant directly for each request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a real-time endpoint with 2 production variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this case, we'll be using both models that were already configured in previous steps. Each one is created as a production variant of the endpoint:\n",
    "- XGBoost Model with 60% of the traffic.\n",
    "- ScikitLearn Model with 40% left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_variants = [\n",
    "    production_variant(\n",
    "        # CODE STARTS HERE\n",
    "        \n",
    "        # CODE ENDS HERE\n",
    "    ),\n",
    "    production_variant(\n",
    "        # CODE STARTS HERE\n",
    "        \n",
    "        # CODE ENDS HERE\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_variant_endpoint = sagemaker_session.endpoint_from_production_variants(\n",
    "    name=f\"DEMO-production-variant-endpoint-{datetime.utcnow():%Y-%m-%d-%H%M%S}\",\n",
    "    production_variants=production_variants,\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each endpoint variant will receive a proportion of the calls, defined by the weight. \n",
    "# A specific variant can be called by passing the 'TargetVariant' parameter\n",
    "\n",
    "def invoke_endpoint(payload, **kwargs):\n",
    "    response = sagemaker_runtime_client.invoke_endpoint(Body=payload, **kwargs)\n",
    "    prediction = response[\"Body\"].read()\n",
    "    variant = response['ResponseMetadata']['HTTPHeaders']['x-amzn-invoked-production-variant']\n",
    "    return prediction, variant\n",
    "\n",
    "print(f\"Sending test traffic to the endpoint {production_variant_endpoint}. \\nPlease wait\\n\", end=\"\")\n",
    "#params = {'EndpointName':production_variant_endpoint, 'ContentType':dataset_type, 'TargetVariant':\"sklearn-variant\",} # You can pass the endpoint variant\n",
    "params = {'EndpointName':production_variant_endpoint, 'ContentType':dataset_type,}\n",
    "with open(test_dataset, \"r\") as f:\n",
    "    for i, row in enumerate(f):\n",
    "        if i < 15:\n",
    "            response, variant = invoke_endpoint(row.rstrip(\"\\n\")[2:], **params)\n",
    "            print('Received prediction :' + str(response) + ' from variant ' + variant)\n",
    "            time.sleep(0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A/B Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, such as e-commerce applications, offline model evaluation isn’t sufficient, and you need to A/B test models in production before making the decision of updating models. With Amazon SageMaker, you can easily perform A/B testing on ML models by running multiple production variants on an endpoint. You can use production variants to test ML models that have been trained using different training datasets, algorithms, and ML frameworks; test how they perform on different instance types; or a combination of all of the above.\n",
    "\n",
    "In A/B testing, you test different variants of your models and compare how each variant performs relative to each other. You then choose the best-performing model to replace a previously-existing model new version delivers better performance than the previously-existing version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see how often each variant is called.\n",
    "\n",
    "cw = boto_session.client(\"cloudwatch\")\n",
    "\n",
    "def get_invocation_metrics_for_endpoint_variant(endpoint_name, variant_name, start_time, end_time):\n",
    "    metrics = cw.get_metric_statistics(\n",
    "        Namespace=\"AWS/SageMaker\",\n",
    "        MetricName=\"Invocations\",\n",
    "        StartTime=start_time,\n",
    "        EndTime=end_time,\n",
    "        Period=60,\n",
    "        Statistics=[\"Sum\"],\n",
    "        Dimensions=[\n",
    "            {\"Name\": \"EndpointName\", \"Value\": endpoint_name},\n",
    "            {\"Name\": \"VariantName\", \"Value\": variant_name},\n",
    "        ],\n",
    "    )\n",
    "    return (\n",
    "        pd.DataFrame(metrics[\"Datapoints\"])\n",
    "        .sort_values(\"Timestamp\")\n",
    "        .set_index(\"Timestamp\")\n",
    "        .drop(\"Unit\", axis=1)\n",
    "        .rename(columns={\"Sum\": variant_name})\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_endpoint_metrics(start_time=None):\n",
    "    start_time = start_time or datetime.now() - timedelta(minutes=60)\n",
    "    end_time = datetime.now()\n",
    "    metrics_variant1 = get_invocation_metrics_for_endpoint_variant(\n",
    "        production_variant_endpoint, \"xgboost-variant\", start_time, end_time\n",
    "    )\n",
    "    metrics_variant2 = get_invocation_metrics_for_endpoint_variant(\n",
    "        production_variant_endpoint, \"sklearn-variant\", start_time, end_time\n",
    "    )\n",
    "    metrics_variants = metrics_variant1.join(metrics_variant2, how=\"outer\")\n",
    "    metrics_variants.plot()\n",
    "    return metrics_variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Waiting a few seconds for initial metric creation...\")\n",
    "time.sleep(30)\n",
    "m = plot_endpoint_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a few calls to a specific variant and re-check the stats\n",
    "\n",
    "params = {'EndpointName':production_variant_endpoint, 'ContentType':dataset_type, 'TargetVariant':\"sklearn-variant\",} # You can pass the endpoint variant\n",
    "with open(test_dataset, \"r\") as f:\n",
    "    for i, row in enumerate(f):\n",
    "        if i < 15:\n",
    "            response, _ = invoke_endpoint(row.rstrip(\"\\n\")[2:], **params)\n",
    "            time.sleep(0.1)\n",
    "time.sleep(10)\n",
    "m = plot_endpoint_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's evaluate the performance of each endpoint by calling it with test data\n",
    "\n",
    "params_sklearn = {'EndpointName':production_variant_endpoint, 'ContentType':dataset_type, 'TargetVariant':\"sklearn-variant\",}\n",
    "params_xgboost = {'EndpointName':production_variant_endpoint, 'ContentType':dataset_type, 'TargetVariant':\"xgboost-variant\",}\n",
    "label = []\n",
    "predict_sklearn = []\n",
    "predict_xgboost = []\n",
    "with open(test_dataset, \"r\") as f:\n",
    "    for i, row in enumerate(f):\n",
    "        if i < 100:\n",
    "            label.append(int(row.rstrip(\"\\n\")[0]))\n",
    "            response, _ = invoke_endpoint(row.rstrip(\"\\n\")[2:], **params_sklearn)\n",
    "            predict_sklearn.append(round(eval(response)['pred']))\n",
    "            response, _ = invoke_endpoint(row.rstrip(\"\\n\")[2:], **params_xgboost)\n",
    "            predict_xgboost.append(round(eval(response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get then some metrics for each of the endpoint with predictions you got from both variants to evaluate both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.array(label)\n",
    "predict_sklearn = np.array(predict_sklearn)\n",
    "predict_xgboost = np.array(predict_xgboost)\n",
    "\n",
    "\n",
    "sklearn_accuracy = sum(predict_sklearn == label) / len(label)\n",
    "xgboost_accuracy = sum(predict_xgboost == label) / len(label)\n",
    "print('Accuracy -> sklearn: {}, xgboost: {}'.format(sklearn_accuracy, xgboost_accuracy))\n",
    "\n",
    "# Calculate precision\n",
    "sklearn_precision = round(sum(predict_sklearn[predict_sklearn == 1] == label[predict_sklearn == 1]) / len(predict_sklearn[predict_sklearn == 1]), 2)\n",
    "xgboost_precision = round(sum(predict_xgboost[predict_xgboost == 1] == label[predict_xgboost == 1]) / len(predict_xgboost[predict_xgboost == 1]), 2)\n",
    "print('Precision -> sklearn: {}, xgboost: {}'.format(sklearn_precision, xgboost_precision))\n",
    "\n",
    "# Calculate recall\n",
    "sklearn_recall = round(sum(predict_sklearn[predict_sklearn == 1] == label[predict_sklearn == 1]) / len(label[label == 1]), 2)\n",
    "xgboost_recall = round(sum(predict_xgboost[predict_xgboost == 1] == label[predict_xgboost == 1]) / len(label[label == 1]), 2)\n",
    "print('Recall -> sklearn: {}, xgboost: {}'.format(sklearn_recall, xgboost_precision))\n",
    "\n",
    "# Calculate F1 score\n",
    "sklearn_f1_score = round(2 * (sklearn_precision * sklearn_recall) / (sklearn_precision + sklearn_recall), 2)\n",
    "xgboost_f1_score = round(2 * (xgboost_precision * xgboost_recall) / (xgboost_precision + xgboost_recall), 2)\n",
    "print('F1 Score -> sklearn: {}, xgboost: {}'.format(sklearn_f1_score, xgboost_f1_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's update the traffic weights accordingly in our endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see that the xgboost variant is performing better, so lets increase the weight given to this variant\n",
    "sagemaker_client.update_endpoint_weights_and_capacities(\n",
    "    EndpointName=production_variant_endpoint,\n",
    "    DesiredWeightsAndCapacities=[\n",
    "        {\"DesiredWeight\": 20, \"VariantName\": \"sklearn-variant\"},\n",
    "        {\"DesiredWeight\": 80, \"VariantName\": \"xgboost-variant\"},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can confirm that the change was succesfull by describing the enpoint\n",
    "{\n",
    "    variant[\"VariantName\"]: variant[\"CurrentWeight\"]\n",
    "    for variant in sagemaker_client.describe_endpoint(EndpointName=production_variant_endpoint)[\"ProductionVariants\"]\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
