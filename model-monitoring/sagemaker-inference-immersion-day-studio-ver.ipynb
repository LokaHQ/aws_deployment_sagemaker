{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS SageMaker Inference Inmmersion Day\n",
    "This notebook shows how to:\n",
    "* __Lab 1:__ Deploy a real time endpoint with a prebuilt container and invoke it.\n",
    "* __Lab 2:__ Deploy a real time endpoint with a custom container.\n",
    "* __Lab 3:__ Host an endpoint with multiple production variants with different traffic.\n",
    "* __Lab 4:__ Autoscaling your endpoint.\n",
    "* __Lab 5:__ Host a machine learning model in Amazon SageMaker and capture inference requests, results, and metadata.\n",
    "* Schedule Clarify bias monitor to monitor predictions for bias drift on a regular basis.\n",
    "* Schedule Clarify explainability monitor to monitor predictions for feature attribution drift on a regular basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Setup\n",
    "\n",
    "To get started, make sure these prerequisites completed.\n",
    "\n",
    "* Configure SageMaker Studio. Create a sagemaker studio user and an IAM role for that user to use. Role should have: `AmazonSageMakerFullAccess` and `AmazonS3FullAccess` attached. Once configured, follow this instructions: https://sagemaker-immersionday.workshop.aws/en/lab3/option1.html#prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Import APIs to be used by the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from sagemaker.session import production_variant\n",
    "from sagemaker import get_execution_role, image_uris, Session\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.clarify import (\n",
    "    BiasConfig,\n",
    "    DataConfig,\n",
    "    ModelConfig,\n",
    "    ModelPredictedLabelConfig,\n",
    "    SHAPConfig,\n",
    ")\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.model_monitor import (\n",
    "    BiasAnalysisConfig,\n",
    "    CronExpressionGenerator,\n",
    "    DataCaptureConfig,\n",
    "    EndpointInput,\n",
    "    ExplainabilityAnalysisConfig,\n",
    "    ModelBiasMonitor,\n",
    "    ModelExplainabilityMonitor,\n",
    ")\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handful of configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoleArn: arn:aws:iam::377983720232:role/SageMakerExecution\n",
      "AWS region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "print(f\"RoleArn: {role}\")\n",
    "\n",
    "boto_session = boto3.session.Session()\n",
    "\n",
    "sagemaker_session = Session(boto_session)\n",
    "sagemaker_client = sagemaker_session.sagemaker_client\n",
    "sagemaker_runtime_client = sagemaker_session.sagemaker_runtime_client\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "print(f\"AWS region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo Bucket: sagemaker-us-east-1-377983720232\n",
      "S3 key: s3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901\n",
      "Capture path: s3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/datacapture\n",
      "Ground truth path: s3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/ground_truth_data/2021-10-22-16-19-56\n",
      "Report path: s3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/reports\n",
      "Baseline results uri: s3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/baselining\n"
     ]
    }
   ],
   "source": [
    "# A different bucket can be used, but make sure the role for this notebook has\n",
    "# the s3:PutObject permissions. This is the bucket into which the data is captured\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "print(f\"Demo Bucket: {bucket}\")\n",
    "prefix = \"sagemaker/DEMO-ClarifyModelMonitor-20200901\"\n",
    "s3_key = f\"s3://{bucket}/{prefix}\"\n",
    "print(f\"S3 key: {s3_key}\")\n",
    "\n",
    "s3_capture_upload_path = f\"{s3_key}/datacapture\"\n",
    "ground_truth_upload_path = f\"{s3_key}/ground_truth_data/{datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    "s3_report_path = f\"{s3_key}/reports\"\n",
    "\n",
    "print(f\"Capture path: {s3_capture_upload_path}\")\n",
    "print(f\"Ground truth path: {ground_truth_upload_path}\")\n",
    "print(f\"Report path: {s3_report_path}\")\n",
    "\n",
    "baseline_results_uri = f\"{s3_key}/baselining\"\n",
    "print(f\"Baseline results uri: {baseline_results_uri}\")\n",
    "\n",
    "endpoint_instance_count = 1\n",
    "endpoint_instance_type = \"ml.m5.large\"\n",
    "schedule_expression = CronExpressionGenerator.hourly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model files and data files\n",
    "\n",
    "The prebuilt model, and a couple of dataset files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"model/xgb-churn-prediction-model.tar.gz\"\n",
    "test_file = \"test_data/test-file.txt\"\n",
    "test_dataset = \"test_data/test.csv\"\n",
    "validation_dataset = \"test_data/validation-dataset-with-header.csv\"\n",
    "dataset_type = \"text/csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(validation_dataset) as f:\n",
    "    headers_line = f.readline().rstrip()\n",
    "all_headers = headers_line.split(\",\")\n",
    "label_header = all_headers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that the execution role for this notebook has the necessary permissions to proceed. Put a simple test object into the S3 bucket speciÔ¨Åed above. If this command fails, update the role to have `s3:PutObject` permission on the bucket and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! We are all set to proceed.\n"
     ]
    }
   ],
   "source": [
    "# Upload a test file\n",
    "S3Uploader.upload(test_file, f\"s3://{bucket}/test_upload\", sagemaker_session=sagemaker_session)\n",
    "print(\"Success! We are all set to proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 1: Deploying a real-time endpoint on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create an endpoint using an already trained model with [XGBoost library](https://github.com/dmlc/xgboost). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the pre-trained model to Amazon S3\n",
    "As an example, this code uploads a pre-trained XGBoost model that is ready for to be deployed. This model was trained using the [code that you can find on training folder](../training/xgboost_customer_churn.ipynb) in SageMaker. In order to deploy an endpoint, we will need to first upload the model artifact (the serialized object) to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file has been uploaded to s3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/xgb-churn-prediction-model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_url = S3Uploader.upload(local_path=model_file, desired_s3_uri=s3_key, sagemaker_session=sagemaker_session)\n",
    "print(f\"Model file has been uploaded to {model_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model to Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with deploying a pre-trained churn prediction model. Here, create the SageMaker `Model` object with the image and model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name:  DEMO-xgb-churn-pred-model-monitor-2021-10-22-2120\n",
      "Endpoint name:  DEMO-xgb-churn-model-monitor-2021-10-22-2120\n"
     ]
    }
   ],
   "source": [
    "model_name = f\"DEMO-xgb-churn-pred-model-monitor-{datetime.utcnow():%Y-%m-%d-%H%M}\"\n",
    "print(\"Model name: \", model_name)\n",
    "endpoint_name = f\"DEMO-xgb-churn-model-monitor-{datetime.utcnow():%Y-%m-%d-%H%M}\"\n",
    "print(\"Endpoint name: \", endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use an already pre-built docker image with xgboost 0.90-1 version by SageMaker. In order to do this, we will retrieve the ECR docker image URL from Amazon ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost image uri: 683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:0.90-1-cpu-py3\n"
     ]
    }
   ],
   "source": [
    "image_uri = image_uris.retrieve(\"xgboost\", region, \"0.90-1\")\n",
    "print(f\"XGBoost image uri: {image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the model, we need to pass the mentioned Docker ECR uri, an IAM role used to access the data on S3 and create the endpoint on SageMaker, the model url on S3. Also in order to deploy an endpoint, we will to configure the instance (count and type) and a serializer that will define how the data will be encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model DEMO-xgb-churn-pred-model-monitor-2021-10-22-2120 to endpoint DEMO-xgb-churn-model-monitor-2021-10-22-2120\n",
      "-------------!"
     ]
    }
   ],
   "source": [
    "model = Model(\n",
    "    role=role,\n",
    "    name=model_name,\n",
    "    image_uri=image_uri,\n",
    "    model_data=model_url,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "print(f\"Deploying model {model_name} to endpoint {endpoint_name}\")\n",
    "model.deploy(\n",
    "    initial_instance_count=endpoint_instance_count,\n",
    "    instance_type=endpoint_instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=CSVSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending test traffic to the endpoint DEMO-xgb-churn-model-monitor-2021-10-22-2120. \n",
      "Please waitb'0.13533151149749756'\n",
      ".b'0.9820775389671326'\n",
      ".b'0.0010247475001960993'\n",
      ".b'0.004050217568874359'\n",
      ".b'0.3928244113922119'\n",
      ".b'0.9721772074699402'\n",
      ".b'0.9912384152412415'\n",
      ".b'0.7279552221298218'\n",
      ".b'0.8881062269210815'\n",
      ".b'0.9909957051277161'\n",
      ".\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sending test traffic to the endpoint {endpoint_name}. \\nPlease wait\", end=\"\")\n",
    "test_dataset_size = 0  # record the number of rows in data we're sending for inference\n",
    "count = 0\n",
    "with open(test_dataset, \"r\") as f:\n",
    "    for row in f:\n",
    "        if test_dataset_size < 10:\n",
    "            payload = row.rstrip(\"\\n\")\n",
    "            response = sagemaker_runtime_client.invoke_endpoint(\n",
    "                EndpointName=endpoint_name,\n",
    "                Body=payload[2:],\n",
    "                ContentType=dataset_type,\n",
    "            )\n",
    "            prediction = response[\"Body\"].read()\n",
    "            print(prediction)\n",
    "            \n",
    "            time.sleep(0.5)\n",
    "        test_dataset_size += 1\n",
    "\n",
    "print()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 2: Deploy custom container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and push your docker image to ECR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will:\n",
    "- Create an ECR repository if does not exist.\n",
    "- Build a docker image and tag it accordingly\n",
    "- Push the docker image that has been built to the created ECR repo.\n",
    "\n",
    "In order to do this from SageMaker studio notebook we need to use `sm-docker`. If you want to replicate this running locally, please refer to `sagemaker_inference_immerssion_day` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker-studio-image-build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image name churn-predictor-sklearn\n",
      "Profile name default\n",
      "{\n",
      "    \"repositories\": [\n",
      "        {\n",
      "            \"repositoryArn\": \"arn:aws:ecr:us-east-1:377983720232:repository/churn-predictor-sklearn\",\n",
      "            \"registryId\": \"377983720232\",\n",
      "            \"repositoryName\": \"churn-predictor-sklearn\",\n",
      "            \"repositoryUri\": \"377983720232.dkr.ecr.us-east-1.amazonaws.com/churn-predictor-sklearn\",\n",
      "            \"createdAt\": \"2021-10-22T14:29:14-05:00\",\n",
      "            \"imageTagMutability\": \"MUTABLE\",\n",
      "            \"imageScanningConfiguration\": {\n",
      "                \"scanOnPush\": false\n",
      "            },\n",
      "            \"encryptionConfiguration\": {\n",
      "                \"encryptionType\": \"AES256\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "Login Succeeded\n",
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                                         \n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.1s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 34B                                           0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/ubuntu:18.04            0.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 34B                                           0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/ubuntu:18.04            0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 34B                                           0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/ubuntu:18.04            0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 34B                                           0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/ubuntu:18.04            0.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.8s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 34B                                           0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/ubuntu:18.04            0.6s\n",
      "\u001b[34m => [auth] library/ubuntu:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.9s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 34B                                           0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/ubuntu:18.04            0.8s\n",
      "\u001b[34m => [auth] library/ubuntu:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.1s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 34B                                           0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/ubuntu:18.04            0.9s\n",
      "\u001b[34m => [auth] library/ubuntu:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.2s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 34B                                           0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/ubuntu:18.04            1.1s\n",
      "\u001b[34m => [auth] library/ubuntu:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.4s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 34B                                           0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/ubuntu:18.04            1.2s\n",
      "\u001b[34m => [auth] library/ubuntu:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.5s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 34B                                           0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/ubuntu:18.04            1.4s\n",
      "\u001b[34m => [auth] library/ubuntu:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.7s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 34B                                           0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/ubuntu:18.04            1.5s\n",
      "\u001b[34m => [auth] library/ubuntu:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.8s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 34B                                           0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/ubuntu:18.04            1.7s\n",
      "\u001b[34m => [auth] library/ubuntu:pull token for registry-1.docker.io              0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.0s (3/4)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 34B                                           0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/ubuntu:18.04            1.8s\n",
      "\u001b[34m => [auth] library/ubuntu:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.1s (6/15)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 34B                                           0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:18.04            1.9s\n",
      "\u001b[0m\u001b[34m => [auth] library/ubuntu:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/10] FROM docker.io/library/ubuntu:18.04@sha256:0fedbd5bd9fb72089c  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/library/ubuntu:18.04@sha256:0fedbd5bd9fb72089c7b  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 1.80kB                                        0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.2s (14/15)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 34B                                           0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:18.04            1.9s\n",
      "\u001b[0m\u001b[34m => [auth] library/ubuntu:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/10] FROM docker.io/library/ubuntu:18.04@sha256:0fedbd5bd9fb72089c  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/library/ubuntu:18.04@sha256:0fedbd5bd9fb72089c7b  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 1.80kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/10] RUN apt-get update &&     apt-get -y upgrade &&     ap  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/10] RUN pip3 install pip --upgrade                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/10] COPY requirements.txt /.                                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/10] RUN pip3 install -r requirements.txt && rm requirement  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/10] RUN ln -sf /dev/stdout /var/log/nginx/access.log        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/10] RUN ln -sf /dev/stderr /var/log/nginx/error.log         0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/10] COPY . /opt/program                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/10] WORKDIR /opt/program                                           0.0s\n",
      "\u001b[0m => [10/10] RUN chmod +x /opt/program/serve                                0.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.4s (14/15)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 34B                                           0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:18.04            1.9s\n",
      "\u001b[0m\u001b[34m => [auth] library/ubuntu:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/10] FROM docker.io/library/ubuntu:18.04@sha256:0fedbd5bd9fb72089c  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/library/ubuntu:18.04@sha256:0fedbd5bd9fb72089c7b  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 1.80kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/10] RUN apt-get update &&     apt-get -y upgrade &&     ap  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/10] RUN pip3 install pip --upgrade                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/10] COPY requirements.txt /.                                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/10] RUN pip3 install -r requirements.txt && rm requirement  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/10] RUN ln -sf /dev/stdout /var/log/nginx/access.log        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/10] RUN ln -sf /dev/stderr /var/log/nginx/error.log         0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/10] COPY . /opt/program                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/10] WORKDIR /opt/program                                           0.0s\n",
      "\u001b[0m => [10/10] RUN chmod +x /opt/program/serve                                0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.5s (14/15)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 34B                                           0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:18.04            1.9s\n",
      "\u001b[0m\u001b[34m => [auth] library/ubuntu:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/10] FROM docker.io/library/ubuntu:18.04@sha256:0fedbd5bd9fb72089c  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/library/ubuntu:18.04@sha256:0fedbd5bd9fb72089c7b  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 1.80kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/10] RUN apt-get update &&     apt-get -y upgrade &&     ap  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/10] RUN pip3 install pip --upgrade                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/10] COPY requirements.txt /.                                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/10] RUN pip3 install -r requirements.txt && rm requirement  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/10] RUN ln -sf /dev/stdout /var/log/nginx/access.log        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/10] RUN ln -sf /dev/stderr /var/log/nginx/error.log         0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/10] COPY . /opt/program                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/10] WORKDIR /opt/program                                           0.0s\n",
      "\u001b[0m => [10/10] RUN chmod +x /opt/program/serve                                0.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.6s (16/16)                                                       \n",
      "\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 34B                                           0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:18.04            1.9s\n",
      "\u001b[0m\u001b[34m => [auth] library/ubuntu:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/10] FROM docker.io/library/ubuntu:18.04@sha256:0fedbd5bd9fb72089c  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/library/ubuntu:18.04@sha256:0fedbd5bd9fb72089c7b  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 1.80kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/10] RUN apt-get update &&     apt-get -y upgrade &&     ap  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/10] RUN pip3 install pip --upgrade                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/10] COPY requirements.txt /.                                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/10] RUN pip3 install -r requirements.txt && rm requirement  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/10] RUN ln -sf /dev/stdout /var/log/nginx/access.log        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/10] RUN ln -sf /dev/stderr /var/log/nginx/error.log         0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/10] COPY . /opt/program                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/10] WORKDIR /opt/program                                           0.0s\n",
      "\u001b[0m\u001b[34m => [10/10] RUN chmod +x /opt/program/serve                                0.3s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.1s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.1s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:994a942e0f404747ae2059b9ceac68bf1e0164f1185b9  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/churn-predictor-sklearn                 0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.6s (16/16) FINISHED                                              \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 37B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 34B                                           0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/ubuntu:18.04            1.9s\n",
      "\u001b[0m\u001b[34m => [auth] library/ubuntu:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/10] FROM docker.io/library/ubuntu:18.04@sha256:0fedbd5bd9fb72089c  0.0s\n",
      "\u001b[0m\u001b[34m => => resolve docker.io/library/ubuntu:18.04@sha256:0fedbd5bd9fb72089c7b  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 1.80kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/10] RUN apt-get update &&     apt-get -y upgrade &&     ap  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/10] RUN pip3 install pip --upgrade                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/10] COPY requirements.txt /.                                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/10] RUN pip3 install -r requirements.txt && rm requirement  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/10] RUN ln -sf /dev/stdout /var/log/nginx/access.log        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/10] RUN ln -sf /dev/stderr /var/log/nginx/error.log         0.0s\n",
      "\u001b[0m\u001b[34m => [ 8/10] COPY . /opt/program                                            0.0s\n",
      "\u001b[0m\u001b[34m => [ 9/10] WORKDIR /opt/program                                           0.0s\n",
      "\u001b[0m\u001b[34m => [10/10] RUN chmod +x /opt/program/serve                                0.3s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.1s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.1s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:994a942e0f404747ae2059b9ceac68bf1e0164f1185b9  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/churn-predictor-sklearn                 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[?25hThe push refers to repository [377983720232.dkr.ecr.us-east-1.amazonaws.com/churn-predictor-sklearn]\n",
      "\n",
      "\u001b[1B645d9b59: Preparing \n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[1B03581aec: Preparing \n",
      "\u001b[1Bcfa943cf: Preparing \n",
      "\u001b[1B2f1df1f6: Preparing \n",
      "\u001b[1Bc5960a73: Preparing \n",
      "\u001b[1Ba3340fcf: Preparing \n",
      "\u001b[1B6dddd61f: Preparing \n",
      "\u001b[1Bb1720133: Preparing \n",
      "\u001b[8B03581aec: Pushed lready exists 7kBA\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[8A\u001b[2Klatest: digest: sha256:5d0df5bd0b1ec42901974cce98425216eb4afe552f266de74f68604657df5f3f size: 2410\n",
      "CPU times: user 432 ms, sys: 173 ms, total: 604 ms\n",
      "Wall time: 17.6 s\n"
     ]
    }
   ],
   "source": [
    "!cd custom_container && sm-docker build . --repository sagemaker-studio-sklearn-custom:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the endpoint using the custom image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: Before executing next cells, assign the image uri/repository in the next cell to the `image_uri_custom` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri_custom = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name:  DEMO-xgb-churn-pred-model-monitor-2021-10-22-2120\n",
      "Endpoint name:  DEMO-xgb-churn-model-monitor-2021-10-22-2120\n"
     ]
    }
   ],
   "source": [
    "model_name_custom = f\"DEMO-sklearn-churn-predictor-{datetime.utcnow():%Y-%m-%d-%H%M}\"\n",
    "print(\"Model name: \", model_name)\n",
    "endpoint_name_custom = f\"DEMO-sklearn-churn-predictor-{datetime.utcnow():%Y-%m-%d-%H%M}\"\n",
    "print(\"Endpoint name: \", endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model DEMO-xgb-churn-pred-model-monitor-2021-10-22-2120 to endpoint DEMO-xgb-churn-model-monitor-2021-10-22-2120\n",
      "-----------!"
     ]
    }
   ],
   "source": [
    "model = Model(\n",
    "    role=role,\n",
    "    name=model_name_custom,\n",
    "    image_uri=image_uri_custom,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "print(f\"Deploying model {model_name} to endpoint {endpoint_name}\")\n",
    "model.deploy(\n",
    "    initial_instance_count=endpoint_instance_count,\n",
    "    instance_type=endpoint_instance_type,\n",
    "    endpoint_name=endpoint_name_custom\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending test traffic to the endpoint DEMO-sklearn-churn-predictor-2021-10-22-2128. \n",
      "Please waitb'{\"pred\":0.5006522625587263}\\n'\n",
      ".b'{\"pred\":0.4974552997917495}\\n'\n",
      ".b'{\"pred\":0.49307747953367825}\\n'\n",
      ".b'{\"pred\":0.4812988038778532}\\n'\n",
      ".b'{\"pred\":0.200031085510429}\\n'\n",
      ".b'{\"pred\":0.18069012264588952}\\n'\n",
      ".b'{\"pred\":0.19594545622434711}\\n'\n",
      ".b'{\"pred\":0.4847784452714544}\\n'\n",
      ".b'{\"pred\":0.2259438174343721}\\n'\n",
      ".b'{\"pred\":0.47800537787375746}\\n'\n",
      ".\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sending test traffic to the endpoint {endpoint_name_custom}. \\nPlease wait\", end=\"\")\n",
    "test_dataset_size = 0  # record the number of rows in data we're sending for inference\n",
    "count = 0\n",
    "with open(test_dataset, \"r\") as f:\n",
    "    for row in f:\n",
    "        if test_dataset_size < 10:\n",
    "            payload = row.rstrip(\"\\n\")\n",
    "            response = sagemaker_runtime_client.invoke_endpoint(\n",
    "                EndpointName=endpoint_name_custom,\n",
    "                Body=payload[2:],\n",
    "                ContentType=dataset_type,\n",
    "            )\n",
    "            prediction = response[\"Body\"].read()\n",
    "            print(prediction)\n",
    "            \n",
    "            time.sleep(0.5)\n",
    "        test_dataset_size += 1\n",
    "\n",
    "print()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 3: Production Variants and A/B Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker enables you to test multiple models or model versions behind the same endpoint using production variants. Each production variant identifies a machine learning (ML) model and the resources deployed for hosting the model. You can distribute endpoint invocation requests across multiple production variants by providing the traffic distribution for each variant, or you can invoke a specific variant directly for each request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a real-time endpoint with 2 production variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this case, we'll be using both models that were already configured in previous steps. Each one is created as a production variant of the endpoint:\n",
    "- XGBoost Model with 80% of the traffic.\n",
    "- ScikitLearn Model with 20% left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_variants = [\n",
    "    production_variant(\n",
    "        model_name=model_name, \n",
    "        instance_type=endpoint_instance_type,\n",
    "        initial_instance_count=endpoint_instance_count,\n",
    "        initial_weight=0.8,\n",
    "        variant_name=\"xgboost-variant\"\n",
    "    ),\n",
    "    production_variant(\n",
    "        model_name=model_name_custom,\n",
    "        instance_type=endpoint_instance_type,\n",
    "        initial_instance_count=endpoint_instance_count,\n",
    "        initial_weight=0.2,\n",
    "        variant_name=\"sklearn-variant\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'production-variant-endpoint-2021-10-22-2136'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "production_variant_endpoint = sagemaker_session.endpoint_from_production_variants(\n",
    "    name=f\"DEMO-production-variant-endpoint-{datetime.utcnow():%Y-%m-%d-%H%M}\",\n",
    "    production_variants=production_variants,\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending test traffic to the endpoint production-variant-endpoint-2021-10-22-2136. \n",
      "Please waitb'0.13533151149749756'\n",
      ".b'0.9820775389671326'\n",
      ".b'0.0010247475001960993'\n",
      ".b'{\"pred\":0.4812988038778532}\\n'\n",
      ".b'0.3928244113922119'\n",
      ".b'0.9721772074699402'\n",
      ".b'0.9912384152412415'\n",
      ".b'{\"pred\":0.4847784452714544}\\n'\n",
      ".b'0.8881062269210815'\n",
      ".b'{\"pred\":0.47800537787375746}\\n'\n",
      ".\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sending test traffic to the endpoint {production_variant_endpoint}. \\nPlease wait\", end=\"\")\n",
    "test_dataset_size = 0  # record the number of rows in data we're sending for inference\n",
    "count = 0\n",
    "with open(test_dataset, \"r\") as f:\n",
    "    for row in f:\n",
    "        if test_dataset_size < 10:\n",
    "            payload = row.rstrip(\"\\n\")\n",
    "            response = sagemaker_runtime_client.invoke_endpoint(\n",
    "                EndpointName=production_variant_endpoint,\n",
    "                Body=payload[2:],\n",
    "                ContentType=dataset_type,\n",
    "            )\n",
    "            prediction = response[\"Body\"].read()\n",
    "            print(prediction)\n",
    "            \n",
    "            time.sleep(0.5)\n",
    "        test_dataset_size += 1\n",
    "\n",
    "print()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A/B Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In A/B testing, you test different variants of your models and compare how each variant performs relative to each other. You then choose the best-performing model to replace a previously-existing model new version delivers better performance than the previously-existing version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References\n",
    "# https://github.com/aws/amazon-sagemaker-examples/tree/master/sagemaker_endpoints/a_b_testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 4: Autoscaling your endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker supports automatic scaling (autoscaling) for your hosted models. Autoscaling dynamically adjusts the number of instances provisioned for a model in response to changes in your workload. When the workload increases, autoscaling brings more instances online. When the workload decreases, autoscaling removes unnecessary instances so that you don't pay for provisioned instances that you aren't using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_auto_scale_client = boto_session.client(\"application-autoscaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's configure autoscaling for the first endpoint from LAB 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_id = f\"endpoint/{endpoint_name}/variant/AllTraffic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, sagemaker endpoint has to be register as a scalable target. A scalable target is a resource that Application Auto Scaling can scale out and scale in. Scalable targets are uniquely identified by the combination of resource ID, scalable dimension, and namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_auto_scale_client.register_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify the metrics and target values for a scaling policy, you configure a target-tracking scaling policy. You can use either a predefined metric or a custom metric.\n",
    "\n",
    "To quickly define a target-tracking scaling policy for a variant, use the `SageMakerVariantInvocationsPerInstance` predefined metric. `SageMakerVariantInvocationsPerInstance` is the average number of times per minute that each instance for a variant is invoked. We strongly recommend using this metric.\n",
    "\n",
    "To use a predefined metric in a scaling policy, create a target tracking configuration for your policy. In the target tracking configuration, include a `PredefinedMetricSpecification` for the predefined metric and a `TargetValue` for the target value of that metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_auto_scale_client.put_scaling_policy(\n",
    "    PolicyName='SageMakerSklearnAutoScalePolicy',\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    PolicyType='TargetTrackingScaling',\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        'TargetValue': 200,\n",
    "        'PredefinedMetricSpecification': {\n",
    "            'PredefinedMetricType': \"SageMakerVariantInvocationsPerInstance\",\n",
    "        },\n",
    "        'DisableScaleIn': False\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TargetValue` can be determined as mentioned here: https://github.com/awsdocs/amazon-sagemaker-developer-guide/blob/master/doc_source/endpoint-scaling-loadtest.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References\n",
    "# https://github.com/awsdocs/amazon-sagemaker-developer-guide/blob/master/doc_source/endpoint-auto-scaling.md\n",
    "# load testing: https://github.com/awsdocs/amazon-sagemaker-developer-guide/blob/master/doc_source/endpoint-scaling-loadtest.md\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/application-autoscaling.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 5: Capturing real-time inference data from Amazon SageMaker endpoints\n",
    "Create an endpoint to showcase the data capture capability in action. (In next parts, model monitors will be created to process the data.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Amazon SageMaker Model Monitor continuously monitors the quality of Amazon SageMaker machine learning models in production. It enables developers to set alerts for when there are deviations in the model quality. Early and pro-active detection of these deviations enables corrective actions, such as retraining models, auditing upstream systems, or fixing data quality issues without having to monitor models manually or build additional tooling. \n",
    "\n",
    "Amazon SageMaker Clarify bias monitoring helps data scientists and ML engineers monitor predictions for bias on a regular basis. One way bias can be introduced in deployed ML models is when the data used in training differs from the data used to generate predictions. This is especially pronounced if the data used for training changes over time (e.g. fluctuating mortgage rates), and the model prediction will not be accurate unless the model is retrained with updated data. For example, model for predicting home prices can be biased if the mortgage rates used to train the model differ from the most current real-world mortgage rate.\n",
    "\n",
    "Aamazon SageMaker Clarify explainability monitoring offers tools to provide global explanations of models and to explain the predictions of a deployed model producing inferences. Such model explanation tools can help ML modelers and developers and other internal stakeholders understand model characteristics as a whole prior to deployment and to debug predictions provided by the model once deployed. The current offering includes a scalable and efficient implementation of [SHAP](https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions), based on the concept of the [Shapley value](https://en.wikipedia.org/wiki/Shapley_value) from the field of cooperative game theory that assigns each feature an importance value for a particular prediction.\n",
    "\n",
    "As the model is monitored, customers can view exportable reports and graphs detailing bias and feature attributions in SageMaker Studio and configure alerts in Amazon CloudWatch to receive notifications if violations are detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file has been uploaded to s3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/xgb-churn-prediction-model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_url = S3Uploader.upload(local_path=model_file, desired_s3_uri=s3_key, sagemaker_session=sagemaker_session)\n",
    "print(f\"Model file has been uploaded to {model_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model to Amazon SageMaker\n",
    "Start with deploying a pre-trained churn prediction model. Here, create the SageMaker `Model` object with the image and model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name:  DEMO-xgb-churn-pred-model-monitor-2021-10-20-1311\n",
      "Endpoint name:  DEMO-xgb-churn-model-monitor-2021-10-20-1311\n"
     ]
    }
   ],
   "source": [
    "model_name = f\"DEMO-xgb-churn-pred-model-monitor-{datetime.utcnow():%Y-%m-%d-%H%M}\"\n",
    "print(\"Model name: \", model_name)\n",
    "endpoint_name = f\"DEMO-xgb-churn-model-monitor-{datetime.utcnow():%Y-%m-%d-%H%M}\"\n",
    "print(\"Endpoint name: \", endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable data capture for monitoring jobs, here specify the new capture option called `DataCaptureConfig`, it enables capturing the request payload and the response payload of the endpoint. The capture config applies to all variants. Go ahead with the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost image uri: 683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:0.90-1-cpu-py3\n",
      "Deploying model DEMO-xgb-churn-pred-model-monitor-2021-10-20-1311 to endpoint DEMO-xgb-churn-model-monitor-2021-10-20-1311\n",
      "----------------!"
     ]
    }
   ],
   "source": [
    "image_uri = image_uris.retrieve(\"xgboost\", region, \"0.90-1\")\n",
    "print(f\"XGBoost image uri: {image_uri}\")\n",
    "model = Model(\n",
    "    role=role,\n",
    "    name=model_name,\n",
    "    image_uri=image_uri,\n",
    "    model_data=model_url,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,\n",
    "    destination_s3_uri=s3_capture_upload_path,\n",
    ")\n",
    "print(f\"Deploying model {model_name} to endpoint {endpoint_name}\")\n",
    "model.deploy(\n",
    "    initial_instance_count=endpoint_instance_count,\n",
    "    instance_type=endpoint_instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    data_capture_config=data_capture_config,\n",
    "    serializer=CSVSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke the deployed model\n",
    "\n",
    "Now send data to this endpoint to get inferences in real time. Because data capture is enabled in the previous steps, the request and response payload, along with some additional metadata, is saved in the Amazon S3 location specified in the DataCaptureConfig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending test traffic to the endpoint DEMO-xgb-churn-model-monitor-2021-10-20-1311. \n",
      "Please waitb'0.13533151149749756'\n",
      ".b'0.9820775389671326'\n",
      ".b'0.0010247475001960993'\n",
      ".b'0.004050217568874359'\n",
      ".b'0.3928244113922119'\n",
      ".b'0.9721772074699402'\n",
      ".b'0.9912384152412415'\n",
      ".b'0.7279552221298218'\n",
      ".b'0.8881062269210815'\n",
      ".b'0.9909957051277161'\n",
      ".b'0.9528167247772217'\n",
      ".b'0.011917267926037312'\n",
      ".b'0.003582812612876296'\n",
      ".b'0.9426030516624451'\n",
      ".b'0.9891262054443359'\n",
      ".b'0.990786612033844'\n",
      ".b'0.9890622496604919'\n",
      ".b'0.0696188285946846'\n",
      ".b'0.9030734896659851'\n",
      ".b'0.9861195087432861'\n",
      ".b'0.9627915024757385'\n",
      ".b'0.0017461476381868124'\n",
      ".b'0.00525158504024148'\n",
      ".b'0.958006739616394'\n",
      ".b'0.9418565034866333'\n",
      ".b'0.167768657207489'\n",
      ".b'0.9907608032226562'\n",
      ".b'0.0028409017249941826'\n",
      ".b'0.9958788156509399'\n",
      ".b'0.01792070083320141'\n",
      ".b'0.05648816376924515'\n",
      ".b'0.030282797291874886'\n",
      ".b'0.9394797086715698'\n",
      ".b'0.005420960485935211'\n",
      ".b'0.004677797667682171'\n",
      ".b'0.001765126595273614'\n",
      ".b'0.5747444033622742'\n",
      ".b'0.46189001202583313'\n",
      ".b'0.982370138168335'\n",
      ".b'0.9460119009017944'\n",
      ".b'0.7241844534873962'\n",
      ".b'0.9790021181106567'\n",
      ".b'0.4356221854686737'\n",
      ".b'0.9477603435516357'\n",
      ".b'0.0019811654929071665'\n",
      ".b'0.9550706148147583'\n",
      ".b'0.0018040258437395096'\n",
      ".b'0.23106655478477478'\n",
      ".b'0.984024167060852'\n",
      ".b'0.0020640583243221045'\n",
      ".b'0.004476564470678568'\n",
      ".b'0.976536214351654'\n",
      ".b'0.9767741560935974'\n",
      ".b'0.23881268501281738'\n",
      ".b'0.08627696335315704'\n",
      ".b'0.9924980998039246'\n",
      ".b'0.05516479164361954'\n",
      ".b'0.0018183188512921333'\n",
      ".b'0.014086469076573849'\n",
      ".b'0.06517886370420456'\n",
      ".b'0.3240712881088257'\n",
      ".b'0.9692743420600891'\n",
      ".b'0.8707166910171509'\n",
      ".b'0.02776174806058407'\n",
      ".b'0.0033817142248153687'\n",
      ".b'0.9865017533302307'\n",
      ".b'0.9965873956680298'\n",
      ".b'0.021161871030926704'\n",
      ".b'0.975571870803833'\n",
      ".b'0.9654720425605774'\n",
      ".b'0.996151864528656'\n",
      ".b'0.9690690040588379'\n",
      ".b'0.9891805052757263'\n",
      ".b'0.002633138792589307'\n",
      ".b'0.3210352957248688'\n",
      ".b'0.010880613699555397'\n",
      ".b'0.026457354426383972'\n",
      ".b'0.9370895028114319'\n",
      ".b'0.006732678972184658'\n",
      ".b'0.25873348116874695'\n",
      ".b'0.32441747188568115'\n",
      ".b'0.9286554455757141'\n",
      ".b'0.003312482498586178'\n",
      ".b'0.8908219337463379'\n",
      ".b'0.961377739906311'\n",
      ".b'0.27146482467651367'\n",
      ".b'0.08270609378814697'\n",
      ".b'0.045084550976753235'\n",
      ".b'0.001045746961608529'\n",
      ".b'0.025371501222252846'\n",
      ".b'0.00790762435644865'\n",
      ".b'0.005234608892351389'\n",
      ".b'0.002561788307502866'\n",
      ".b'0.019123123958706856'\n",
      ".b'0.011313045397400856'\n",
      ".b'0.007382397539913654'\n",
      ".b'0.5826998949050903'\n",
      ".b'0.9732788801193237'\n",
      ".b'0.009226380847394466'\n",
      ".b'0.8471701145172119'\n",
      ".b'0.9903301000595093'\n",
      ".b'0.3555954694747925'\n",
      ".b'0.32011526823043823'\n",
      ".b'0.013992313295602798'\n",
      ".b'0.14217722415924072'\n",
      ".b'0.6044306755065918'\n",
      ".b'0.7181485891342163'\n",
      ".b'0.005391065962612629'\n",
      ".b'0.9807353615760803'\n",
      ".b'0.7375830411911011'\n",
      ".b'0.050323646515607834'\n",
      ".b'0.016494307667016983'\n",
      ".b'0.9046522974967957'\n",
      ".b'0.9816442728042603'\n",
      ".b'0.7907162308692932'\n",
      ".b'0.9858980178833008'\n",
      ".b'0.7624249458312988'\n",
      ".b'0.9843775033950806'\n",
      ".b'0.0026342598721385'\n",
      ".b'0.0043899849988520145'\n",
      ".\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sending test traffic to the endpoint {endpoint_name}. \\nPlease wait\", end=\"\")\n",
    "test_dataset_size = 0  # record the number of rows in data we're sending for inference\n",
    "count = 0\n",
    "with open(test_dataset, \"r\") as f:\n",
    "    for row in f:\n",
    "        if test_dataset_size < 120:\n",
    "            payload = row.rstrip(\"\\n\")\n",
    "            response = sagemaker_runtime_client.invoke_endpoint(\n",
    "                EndpointName=endpoint_name,\n",
    "                Body=payload[2:],\n",
    "                ContentType=dataset_type,\n",
    "            )\n",
    "            prediction = response[\"Body\"].read()\n",
    "            print(prediction)\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            \n",
    "            time.sleep(0.5)\n",
    "        test_dataset_size += 1\n",
    "\n",
    "print()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View captured data\n",
    "\n",
    "Now list the data capture files stored in Amazon S3. There should be different files from different time periods organized based on the hour in which the invocation occurred. The format of the Amazon S3 path is:\n",
    "\n",
    "`s3://{destination-bucket-prefix}/{endpoint-name}/{variant-name}/yyyy/mm/dd/hh/filename.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting 30 seconds for captures to show up\n",
      "Found Capture Files:\n",
      "s3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/datacapture/DEMO-xgb-churn-model-monitor-2021-10-20-1311/AllTraffic/2021/10/20/13/21-26-777-7e346469-d429-4024-ad7f-bdcf3d04e67d.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(\"Waiting 30 seconds for captures to show up\", end=\"\")\n",
    "for _ in range(30):\n",
    "    capture_files = sorted(S3Downloader.list(f\"{s3_capture_upload_path}/{endpoint_name}\", sagemaker_session=sagemaker_session))\n",
    "    if capture_files:\n",
    "        break\n",
    "    print(\".\", end=\"\", flush=True)\n",
    "    time.sleep(1)\n",
    "print()\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files[-5:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, view the content of a single capture file. Take a quick peek at the first few lines in the captured file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"127,400,7.526318546400898,3,2.8669976051386983,0,4.714480597588896,50,6.081809102028246,3,4,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"0.9732788801193237\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"f9c7b132-3471-4ef7-9e62-d89bca8d93c4\",\"inferenceTime\":\"2021-10-20T13:22:26Z\"},\"eventVersion\":\"0\"}\n"
     ]
    }
   ],
   "source": [
    "capture_file = S3Downloader.read_file(capture_files[-1], sagemaker_session=sagemaker_session).split(\"\\n\")[-10:-1]\n",
    "print(capture_file[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the contents of a single line is present below in a formatted JSON file to observe a little better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"captureData\": {\n",
      "    \"endpointInput\": {\n",
      "      \"observedContentType\": \"text/csv\",\n",
      "      \"mode\": \"INPUT\",\n",
      "      \"data\": \"127,400,7.526318546400898,3,2.8669976051386983,0,4.714480597588896,50,6.081809102028246,3,4,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    },\n",
      "    \"endpointOutput\": {\n",
      "      \"observedContentType\": \"text/csv; charset=utf-8\",\n",
      "      \"mode\": \"OUTPUT\",\n",
      "      \"data\": \"0.9732788801193237\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    }\n",
      "  },\n",
      "  \"eventMetadata\": {\n",
      "    \"eventId\": \"f9c7b132-3471-4ef7-9e62-d89bca8d93c4\",\n",
      "    \"inferenceTime\": \"2021-10-20T13:22:26Z\"\n",
      "  },\n",
      "  \"eventVersion\": \"0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(json.loads(capture_file[-1]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start generating some artificial traffic\n",
    "The cell below starts a thread to send some traffic to the endpoint. If there is no traffic, the monitoring jobs are marked as `Failed` since there is no data to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "\n",
    "class WorkerThread(threading.Thread):\n",
    "    def __init__(self, do_run, *args, **kwargs):\n",
    "        super(WorkerThread, self).__init__(*args, **kwargs)\n",
    "        self.__do_run = do_run\n",
    "        self.__terminate_event = threading.Event()\n",
    "\n",
    "    def terminate(self):\n",
    "        self.__terminate_event.set()\n",
    "\n",
    "    def run(self):\n",
    "        while not self.__terminate_event.is_set():\n",
    "            self.__do_run(self.__terminate_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_endpoint(terminate_event):\n",
    "    with open(test_dataset, \"r\") as f:\n",
    "        i = 0\n",
    "        for row in f:\n",
    "            payload = row.rstrip(\"\\n\")\n",
    "            response = sagemaker_runtime_client.invoke_endpoint(\n",
    "                EndpointName=endpoint_name,\n",
    "                ContentType=\"text/csv\",\n",
    "                Body=payload[2:],\n",
    "                InferenceId=str(i),  # unique ID per row\n",
    "            )\n",
    "            i += 1\n",
    "            response[\"Body\"].read()\n",
    "            time.sleep(1)\n",
    "            if terminate_event.is_set():\n",
    "                break\n",
    "\n",
    "\n",
    "# Keep invoking the endpoint with test data\n",
    "invoke_endpoint_thread = WorkerThread(do_run=invoke_endpoint)\n",
    "invoke_endpoint_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the `inferenceId` attribute used above to invoke. If this is present, it will be used to join with ground truth data (otherwise `eventId` will be used):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start generating some fake ground truth\n",
    "\n",
    "Besides captures, model bias monitoring execution also requires ground truth data. In real use cases, ground truth data should be regularly collected and uploaded to designated S3 location. In this example notebook, below code snippet is used to generate fake ground truth data. The first-party merge container will combine captures and ground truth data, and the merged data will be passed to model bias monitoring job for analysis. Similar to captures, the model bias monitoring execution will fail if there's no data to merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_with_id(inference_id):\n",
    "    random.seed(inference_id)  # to get consistent results\n",
    "    rand = random.random()\n",
    "    # format required by the merge container\n",
    "    return {\n",
    "        \"groundTruthData\": {\n",
    "            \"data\": \"1\" if rand < 0.7 else \"0\",  # randomly generate positive labels 70% of the time\n",
    "            \"encoding\": \"CSV\",\n",
    "        },\n",
    "        \"eventMetadata\": {\n",
    "            \"eventId\": str(inference_id),\n",
    "        },\n",
    "        \"eventVersion\": \"0\",\n",
    "    }\n",
    "\n",
    "\n",
    "def upload_ground_truth(upload_time):\n",
    "    records = [ground_truth_with_id(i) for i in range(test_dataset_size)]\n",
    "    fake_records = [json.dumps(r) for r in records]\n",
    "    data_to_upload = \"\\n\".join(fake_records)\n",
    "    target_s3_uri = f\"{ground_truth_upload_path}/{upload_time:%Y/%m/%d/%H/%M%S}.jsonl\"\n",
    "    print(f\"Uploading {len(fake_records)} records to\", target_s3_uri)\n",
    "    S3Uploader.upload_string_as_file_body(data_to_upload, target_s3_uri, sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading 500 records to s3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/ground_truth_data/2021-10-20-08-11-20/2021/10/20/12/2339.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Generate data for the last hour\n",
    "upload_ground_truth(datetime.utcnow() - timedelta(hours=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading 500 records to s3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/ground_truth_data/2021-10-20-08-11-20/2021/10/20/13/2343.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Generate data once a hour\n",
    "def generate_fake_ground_truth(terminate_event):\n",
    "    upload_ground_truth(datetime.utcnow())\n",
    "    for _ in range(0, 60):\n",
    "        time.sleep(60)\n",
    "        if terminate_event.is_set():\n",
    "            break\n",
    "\n",
    "\n",
    "ground_truth_thread = WorkerThread(do_run=generate_fake_ground_truth)\n",
    "ground_truth_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART B: Model Bias Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model bias monitor can detect bias drift of Machine Learning models in a regular basis. Similar to the other monitoring types, the standard procedure of creating a model bias monitor is first baselining and then monitoring schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bias_monitor = ModelBiasMonitor(\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    max_runtime_in_seconds=1800,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Create a baselining job\n",
    "\n",
    "A baselining job runs predictions on training dataset and suggests constraints. `suggest_baseline()` method starts a `SageMakerClarifyProcessor` processing job using SageMaker Clarify container to generate the constraints.\n",
    "\n",
    "The step is not mandatory, but providing constraints file to the monitor can enable violations file generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations\n",
    "\n",
    "Information about the input data need to be provided to the processor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataConfig` stores information about the dataset to be analyzed, for example the dataset file, its format (CSV or JSONLines), headers (if any) and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bias_baselining_job_result_uri = f\"{baseline_results_uri}/model_bias\"\n",
    "model_bias_data_config = DataConfig(\n",
    "    s3_data_input_path=validation_dataset,\n",
    "    s3_output_path=model_bias_baselining_job_result_uri,\n",
    "    label=label_header,\n",
    "    headers=all_headers,\n",
    "    dataset_type=dataset_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BiasConfig` is the configuration of the sensitive groups in the dataset. Typically, bias is measured by computing a metric and comparing it across groups. The group of interest is specified using the \"facet.\" For post-training bias, the possitive label should also be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bias_config = BiasConfig(\n",
    "    label_values_or_threshold=[1],\n",
    "    facet_name=\"Account Length\",\n",
    "    facet_values_or_threshold=[100],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ModelPredictedLabelConfig` specifies how to extract a predicted label from the model output. This model returns probability that user will churn. Here choose an arbitrary 0.8 cutoff to consider that a customer will churn. For more complicated outputs, there are a few more options, like \"label\" is the index, name or JSONpath to locate predicted label in endpoint response payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predicted_label_config = ModelPredictedLabelConfig(\n",
    "    probability_threshold=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ModelConfig` is configuration related to model to be used for inferencing. In order to compute post-training bias metrics, the computation needs to get inferences for the model name provided. To accomplish this, the processing job will use the model to create an ephemeral endpoint (also known as \"shadow endpoint\"). The processing job will delete the shadow endpoint after the computations are completed. The configuration is also used by explainability monitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = ModelConfig(\n",
    "    model_name=model_name,\n",
    "    instance_count=endpoint_instance_count,\n",
    "    instance_type=endpoint_instance_type,\n",
    "    content_type=dataset_type,\n",
    "    accept_type=dataset_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kick off baselining job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  baseline-suggestion-job-2021-10-20-13-24-04-311\n",
      "Inputs:  [{'InputName': 'dataset', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-377983720232/baseline-suggestion-job-2021-10-20-13-24-04-311/input/dataset/validation-dataset-with-header.csv', 'LocalPath': '/opt/ml/processing/input/data', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'analysis_config', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/baselining/model_bias/analysis_config.json', 'LocalPath': '/opt/ml/processing/input/config', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'analysis_result', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/baselining/model_bias', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      "ModelBiasMonitor baselining job: baseline-suggestion-job-2021-10-20-13-24-04-311\n"
     ]
    }
   ],
   "source": [
    "model_bias_monitor.suggest_baseline(\n",
    "    data_config=model_bias_data_config,\n",
    "    model_config=model_config,\n",
    "    \n",
    "    bias_config=model_bias_config,\n",
    "    model_predicted_label_config=model_predicted_label_config,\n",
    ")\n",
    "print(f\"ModelBiasMonitor baselining job: {model_bias_monitor.latest_baselining_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell waits until the baselining job is completed and then inspects the suggested constraints. This step can be skipped, because the monitor to be scheduled will automatically pick up baselining job name and wait for it before monitoring execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............................................................................................!\n",
      "ModelBiasMonitor suggested constraints: s3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/baselining/model_bias/analysis.json\n",
      "{\n",
      "    \"version\": \"1.0\",\n",
      "    \"post_training_bias_metrics\": {\n",
      "        \"label\": \"Churn?_True.\",\n",
      "        \"facets\": {\n",
      "            \"Account Length\": [\n",
      "                {\n",
      "                    \"value_or_threshold\": \"(100, 200]\",\n",
      "                    \"metrics\": [\n",
      "                        {\n",
      "                            \"name\": \"AD\",\n",
      "                            \"description\": \"Accuracy Difference (AD)\",\n",
      "                            \"value\": -0.016047767119285683\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"CDDPL\",\n",
      "                            \"description\": \"Conditional Demographic Disparity in Predicted Labels (CDDPL)\",\n",
      "                            \"value\": null,\n",
      "                            \"error\": \"Group variable is empty or not provided\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"DAR\",\n",
      "                            \"description\": \"Difference in Acceptance Rates (DAR)\",\n",
      "                            \"value\": -0.0060905515780982356\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"DCA\",\n",
      "                            \"description\": \"Difference in Conditional Acceptance (DCA)\",\n",
      "                            \"value\": 0.011011394574972355\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"DCR\",\n",
      "                            \"description\": \"Difference in Conditional Rejection (DCR)\",\n",
      "                            \"value\": 0.02252111354394748\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"DI\",\n",
      "                            \"description\": \"Disparate Impact (DI)\",\n",
      "                            \"value\": 0.9382674150459036\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"DPPL\",\n",
      "                            \"description\": \"Difference in Positive Proportions in Predicted Labels (DPPL)\",\n",
      "                            \"value\": 0.02838974063446703\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"DRR\",\n",
      "                            \"description\": \"Difference in Rejection Rates (DRR)\",\n",
      "                            \"value\": 0.029897820873735825\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"FT\",\n",
      "                            \"description\": \"Flip Test (FT)\",\n",
      "                            \"value\": -0.028629856850715747\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"RD\",\n",
      "                            \"description\": \"Recall Difference (RD)\",\n",
      "                            \"value\": -0.013443370948232225\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"TE\",\n",
      "                            \"description\": \"Treatment Equality (TE)\",\n",
      "                            \"value\": 0.8571428571428568\n",
      "                        }\n",
      "                    ]\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        \"label_value_or_threshold\": \"1\"\n",
      "    },\n",
      "    \"pre_training_bias_metrics\": {\n",
      "        \"label\": \"Churn?_True.\",\n",
      "        \"facets\": {\n",
      "            \"Account Length\": [\n",
      "                {\n",
      "                    \"value_or_threshold\": \"(100, 200]\",\n",
      "                    \"metrics\": [\n",
      "                        {\n",
      "                            \"name\": \"CDDL\",\n",
      "                            \"description\": \"Conditional Demographic Disparity in Labels (CDDL)\",\n",
      "                            \"value\": null,\n",
      "                            \"error\": \"Group variable is empty or not provided\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"CI\",\n",
      "                            \"description\": \"Class Imbalance (CI)\",\n",
      "                            \"value\": 0.022\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"DPL\",\n",
      "                            \"description\": \"Difference in Positive Proportions in Labels (DPL)\",\n",
      "                            \"value\": 0.03749014523029148\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"JS\",\n",
      "                            \"description\": \"Jensen-Shannon Divergence (JS)\",\n",
      "                            \"value\": 0.000703298387291728\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"KL\",\n",
      "                            \"description\": \"Kullback-Liebler Divergence (KL)\",\n",
      "                            \"value\": 0.0028122221770651296\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"KS\",\n",
      "                            \"description\": \"Kolmogorov-Smirnov Distance (KS)\",\n",
      "                            \"value\": 0.03749014523029148\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"LP\",\n",
      "                            \"description\": \"L-p Norm (LP)\",\n",
      "                            \"value\": 0.05301907184001522\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"TVD\",\n",
      "                            \"description\": \"Total Variation Distance (TVD)\",\n",
      "                            \"value\": 0.03749014523029148\n",
      "                        }\n",
      "                    ]\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        \"label_value_or_threshold\": \"1\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model_bias_monitor.latest_baselining_job.wait(logs=False)\n",
    "model_bias_constraints = model_bias_monitor.suggested_constraints()\n",
    "print()\n",
    "print(f\"ModelBiasMonitor suggested constraints: {model_bias_constraints.file_s3_uri}\")\n",
    "print(S3Downloader.read_file(model_bias_constraints.file_s3_uri, sagemaker_session=sagemaker_session))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedule model bias monitor\n",
    "\n",
    "With above constraints collected, now call `create_monitoring_schedule()` method to schedule a hourly monitor, to analyze the data with monitoring schedule. If a baselining job has been submitted, then the monitor will automatically pick up analysis configuration from the baselining job. But if the baselining step is skipped, or the capture dataset has different nature than the training dataset, then analysis configuration has to be provided.\n",
    "\n",
    "`BiasAnalysisConfig` is a subset of the configuration of the baselining job, many options are not needed because,\n",
    "* Model bias monitor will merge captures and ground truth data and use merged data as dataset. (~~DataConfig~~)\n",
    "* Captures already include predictions, so there is no need to create shadow endpoint. (~~ModelConfig~~)\n",
    "* Attributes like probability threshold are provided as part of EndpointInput. (~~ModelPredictedLabelConfig~~)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model bias monitoring schedule: monitoring-schedule-2021-10-20-13-32-24-036\n"
     ]
    }
   ],
   "source": [
    "model_bias_analysis_config = None\n",
    "if not model_bias_monitor.latest_baselining_job:\n",
    "    model_bias_analysis_config = BiasAnalysisConfig(\n",
    "        model_bias_config,\n",
    "        headers=all_headers,\n",
    "        label=label_header,\n",
    "    )\n",
    "model_bias_monitor.create_monitoring_schedule(\n",
    "    analysis_config=model_bias_analysis_config,\n",
    "    output_s3_uri=s3_report_path,\n",
    "    endpoint_input=EndpointInput(\n",
    "        endpoint_name=endpoint_name,\n",
    "        destination=\"/opt/ml/processing/input/endpoint\",\n",
    "        start_time_offset=\"-PT1H\",\n",
    "        end_time_offset=\"-PT0H\",\n",
    "        probability_threshold_attribute=0.8,\n",
    "    ),\n",
    "    ground_truth_input=ground_truth_upload_path,\n",
    "    schedule_cron_expression=schedule_expression,\n",
    ")\n",
    "print(f\"Model bias monitoring schedule: {model_bias_monitor.monitoring_schedule_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for the first execution\n",
    "\n",
    "The schedule starts jobs at the previously specified intervals. Code below wait util time crosses the hour boundary (in UTC) to see executions kick off.\n",
    "\n",
    "Note: Even for an hourly schedule, Amazon SageMaker has a buffer period of 20 minutes to schedule executions. The execution might start in anywhere from zero to ~20 minutes from the hour boundary. This is expected and done for load balancing in the backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_execution_to_start(model_monitor):\n",
    "    print(\n",
    "        \"A hourly schedule was created above and it will kick off executions ON the hour (plus 0 - 20 min buffer).\"\n",
    "    )\n",
    "\n",
    "    print(\"Waiting for the first execution to happen\", end=\"\")\n",
    "    schedule_desc = model_monitor.describe_schedule()\n",
    "    while \"LastMonitoringExecutionSummary\" not in schedule_desc:\n",
    "        schedule_desc = model_monitor.describe_schedule()\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        time.sleep(60)\n",
    "    print()\n",
    "    print(\"Done! Execution has been created\")\n",
    "\n",
    "    print(\"Now waiting for execution to start\", end=\"\")\n",
    "    while schedule_desc[\"LastMonitoringExecutionSummary\"][\"MonitoringExecutionStatus\"] in \"Pending\":\n",
    "        schedule_desc = model_monitor.describe_schedule()\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        time.sleep(10)\n",
    "\n",
    "    print()\n",
    "    print(\"Done! Execution has started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A hourly schedule was created above and it will kick off executions ON the hour (plus 0 - 20 min buffer).\n",
      "Waiting for the first execution to happen.............................\n",
      "Done! Execution has been created\n",
      "Now waiting for execution to start.\n",
      "Done! Execution has started\n"
     ]
    }
   ],
   "source": [
    "wait_for_execution_to_start(model_bias_monitor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real world, a monitoring schedule is supposed to be active all the time. But in this example, it can be stopped to avoid incurring extra charges. A stopped schedule will not trigger further executions, but the ongoing execution will continue. And if needed, the schedule can be restarted by `start_monitoring_schedule()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping Monitoring Schedule with name: monitoring-schedule-2021-10-20-13-32-24-036\n"
     ]
    }
   ],
   "source": [
    "model_bias_monitor.stop_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for the execution to finish\n",
    "\n",
    "In the previous cell, the first execution has started. This section waits for the execution to finish so that its analysis results are available. Here are the possible terminal states and what each of them mean:\n",
    "\n",
    "* Completed - This means the monitoring execution completed and no issues were found in the violations report.\n",
    "* CompletedWithViolations - This means the execution completed, but constraint violations were detected.\n",
    "* Failed - The monitoring execution failed, maybe due to client error (perhaps incorrect role permissions) or infrastructure issues. Further examination of FailureReason and ExitMessage is necessary to identify what exactly happened.\n",
    "* Stopped - job exceeded max runtime or was manually stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waits for the schedule to have last execution in a terminal status.\n",
    "def wait_for_execution_to_finish(model_monitor):\n",
    "    schedule_desc = model_monitor.describe_schedule()\n",
    "    execution_summary = schedule_desc.get(\"LastMonitoringExecutionSummary\")\n",
    "    if execution_summary is not None:\n",
    "        print(\"Waiting for execution to finish\", end=\"\")\n",
    "        while execution_summary[\"MonitoringExecutionStatus\"] not in [\n",
    "            \"Completed\",\n",
    "            \"CompletedWithViolations\",\n",
    "            \"Failed\",\n",
    "            \"Stopped\",\n",
    "        ]:\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            time.sleep(60)\n",
    "            schedule_desc = model_monitor.describe_schedule()\n",
    "            execution_summary = schedule_desc[\"LastMonitoringExecutionSummary\"]\n",
    "        print()\n",
    "        print(\"Done! Execution has finished\")\n",
    "    else:\n",
    "        print(\"Last execution not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for execution to finish..........\n",
      "Done! Execution has finished\n"
     ]
    }
   ],
   "source": [
    "wait_for_execution_to_finish(model_bias_monitor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect execution results\n",
    "\n",
    "List the generated reports,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report URI: s3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/reports/DEMO-xgb-churn-model-monitor-2021-10-20-1311/monitoring-schedule-2021-10-20-13-32-24-036/2021/10/20/14\n",
      "Found Report Files:\n",
      "s3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/reports/DEMO-xgb-churn-model-monitor-2021-10-20-1311/monitoring-schedule-2021-10-20-13-32-24-036/2021/10/20/14/analysis.json\n",
      " s3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/reports/DEMO-xgb-churn-model-monitor-2021-10-20-1311/monitoring-schedule-2021-10-20-13-32-24-036/2021/10/20/14/constraint_violations.json\n",
      " s3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/reports/DEMO-xgb-churn-model-monitor-2021-10-20-1311/monitoring-schedule-2021-10-20-13-32-24-036/2021/10/20/14/report.html\n",
      " s3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/reports/DEMO-xgb-churn-model-monitor-2021-10-20-1311/monitoring-schedule-2021-10-20-13-32-24-036/2021/10/20/14/report.ipynb\n",
      " s3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/reports/DEMO-xgb-churn-model-monitor-2021-10-20-1311/monitoring-schedule-2021-10-20-13-32-24-036/2021/10/20/14/report.pdf\n"
     ]
    }
   ],
   "source": [
    "schedule_desc = model_bias_monitor.describe_schedule()\n",
    "execution_summary = schedule_desc.get(\"LastMonitoringExecutionSummary\")\n",
    "if execution_summary and execution_summary[\"MonitoringExecutionStatus\"] in [\n",
    "    \"Completed\",\n",
    "    \"CompletedWithViolations\",\n",
    "]:\n",
    "    last_model_bias_monitor_execution = model_bias_monitor.list_executions()[-1]\n",
    "    last_model_bias_monitor_execution_report_uri = (\n",
    "        last_model_bias_monitor_execution.output.destination\n",
    "    )\n",
    "    print(f\"Report URI: {last_model_bias_monitor_execution_report_uri}\")\n",
    "    last_model_bias_monitor_execution_report_files = sorted(\n",
    "        S3Downloader.list(last_model_bias_monitor_execution_report_uri, sagemaker_session=sagemaker_session)\n",
    "    )\n",
    "    print(\"Found Report Files:\")\n",
    "    print(\"\\n \".join(last_model_bias_monitor_execution_report_files))\n",
    "else:\n",
    "    last_model_bias_monitor_execution = None\n",
    "    print(\n",
    "        \"====STOP==== \\n No completed executions to inspect further. Please wait till an execution completes or investigate previously reported failures.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are violations compared to the baseline, they will be listed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'version': '1.0', 'violations': [{'facet': 'Account Length', 'facet_value': '(100, 200]', 'metric_name': 'CI', 'constraint_check_type': 'bias_drift_check', 'description': 'Value -0.025038323965252938 does not meet the constraint requirement'}, {'facet': 'Account Length', 'facet_value': '(100, 200]', 'metric_name': 'AD', 'constraint_check_type': 'bias_drift_check', 'description': 'Value -0.08225951077584859 does not meet the constraint requirement'}, {'facet': 'Account Length', 'facet_value': '(100, 200]', 'metric_name': 'DAR', 'constraint_check_type': 'bias_drift_check', 'description': 'Value -0.09409876451046995 does not meet the constraint requirement'}, {'facet': 'Account Length', 'facet_value': '(100, 200]', 'metric_name': 'DCA', 'constraint_check_type': 'bias_drift_check', 'description': 'Value 0.18022843492217522 does not meet the constraint requirement'}, {'facet': 'Account Length', 'facet_value': '(100, 200]', 'metric_name': 'DI', 'constraint_check_type': 'bias_drift_check', 'description': 'Value 1.1351558403807651 does not meet the constraint requirement'}, {'facet': 'Account Length', 'facet_value': '(100, 200]', 'metric_name': 'DPPL', 'constraint_check_type': 'bias_drift_check', 'description': 'Value -0.0519939134378834 does not meet the constraint requirement'}, {'facet': 'Account Length', 'facet_value': '(100, 200]', 'metric_name': 'DRR', 'constraint_check_type': 'bias_drift_check', 'description': 'Value 0.034861527792434954 does not meet the constraint requirement'}, {'facet': 'Account Length', 'facet_value': '(100, 200]', 'metric_name': 'FT', 'constraint_check_type': 'bias_drift_check', 'description': 'Value 0.036889332003988036 does not meet the constraint requirement'}, {'facet': 'Account Length', 'facet_value': '(100, 200]', 'metric_name': 'RD', 'constraint_check_type': 'bias_drift_check', 'description': 'Value -0.09497276916631753 does not meet the constraint requirement'}]}\n"
     ]
    }
   ],
   "source": [
    "if last_model_bias_monitor_execution:\n",
    "    model_bias_violations = last_model_bias_monitor_execution.constraint_violations()\n",
    "    if model_bias_violations:\n",
    "        print(model_bias_violations.body_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis results and CloudWatch metrics are visualized in SageMaker Studio. Select the Endpoints tab, then double click the endpoint to show the UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART C: Model Explainability Monitor\n",
    "\n",
    "Model explainability monitor can explain the predictions of a deployed model producing inferences and detect feature attribution drift on a regular basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_explainability_monitor = ModelExplainabilityMonitor(\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    max_runtime_in_seconds=1800,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a baselining job\n",
    "\n",
    "Similary, a baselining job can be scheduled to suggest constraints for model explainability monitor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the explainability baselining job shares the test dataset with the bias baselining job, so here it uses the same `DataConfig`, the only difference is the job output URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_explainability_baselining_job_result_uri = f\"{baseline_results_uri}/model_explainability\"\n",
    "model_explainability_data_config = DataConfig(\n",
    "    s3_data_input_path=validation_dataset,\n",
    "    s3_output_path=model_explainability_baselining_job_result_uri,\n",
    "    label=label_header,\n",
    "    headers=all_headers,\n",
    "    dataset_type=dataset_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the Clarify explainer offers a scalable and efficient implementation of SHAP, so the explainability config is `SHAPConfig`, including\n",
    "* baseline: A list of rows (at least one) or S3 object URI to be used as the baseline dataset in the Kernel SHAP algorithm. The format should be the same as the dataset format. Each row should contain only the feature columns/values and omit the label column/values.\n",
    "* num_samples: Number of samples to be used in the Kernel SHAP algorithm. This number determines the size of the generated synthetic dataset to compute the SHAP values.\n",
    "* agg_method: Aggregation method for global SHAP values. Valid values are\n",
    "  * \"mean_abs\" (mean of absolute SHAP values for all instances),\n",
    "  * \"median\" (median of SHAP values for all instances) and\n",
    "  * \"mean_sq\" (mean of squared SHAP values for all instances).\n",
    "* use_logit: Indicator of whether the logit function is to be applied to the model predictions. Default is False. If \"use_logit\" is true then the SHAP values will have log-odds units.\n",
    "* save_local_shap_values (bool): Indicator of whether to save the local SHAP values in the output location. Default is True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here use the mean value of test dataset as SHAP baseline\n",
    "test_dataframe = pd.read_csv(test_dataset, header=None)\n",
    "#\n",
    "test_dataframe = test_dataframe.iloc[:, 1:]\n",
    "shap_baseline = [list(test_dataframe.mean())]\n",
    "shap_config = SHAPConfig(\n",
    "    baseline=shap_baseline,\n",
    "    num_samples=100,\n",
    "    agg_method=\"mean_abs\",\n",
    "    save_local_shap_values=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kick off baselining job\n",
    "\n",
    "The same model_config is required, because the explainability baselining job needs to create shadow endpoint to get predictions for generated synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  baseline-suggestion-job-2021-10-20-14-53-49-190\n",
      "Inputs:  [{'InputName': 'dataset', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-377983720232/baseline-suggestion-job-2021-10-20-14-53-49-190/input/dataset/validation-dataset-with-header.csv', 'LocalPath': '/opt/ml/processing/input/data', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'analysis_config', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/baselining/model_explainability/analysis_config.json', 'LocalPath': '/opt/ml/processing/input/config', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'analysis_result', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/baselining/model_explainability', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      "ModelExplainabilityMonitor baselining job: baseline-suggestion-job-2021-10-20-14-53-49-190\n"
     ]
    }
   ],
   "source": [
    "model_explainability_monitor.suggest_baseline(\n",
    "    data_config=model_explainability_data_config,\n",
    "    model_config=model_config,\n",
    "    explainability_config=shap_config,\n",
    ")\n",
    "print(\n",
    "    f\"ModelExplainabilityMonitor baselining job: {model_explainability_monitor.latest_baselining_job_name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for baselining job to finish (or skip this cell because the monitor to be scheduled will wait for it anyway)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................................................................!\n",
      "ModelExplainabilityMonitor suggested constraints: s3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/baselining/model_explainability/analysis.json\n",
      "{\n",
      "    \"version\": \"1.0\",\n",
      "    \"explanations\": {\n",
      "        \"kernel_shap\": {\n",
      "            \"label0\": {\n",
      "                \"global_shap_values\": {\n",
      "                    \"Account Length\": 0.019833173767227454,\n",
      "                    \"VMail Message\": 0.040171186723597714,\n",
      "                    \"Day Mins\": 0.07051106213844671,\n",
      "                    \"Day Calls\": 0.03259263850259535,\n",
      "                    \"Eve Mins\": 0.09491578343919928,\n",
      "                    \"Eve Calls\": 0.026107119787124954,\n",
      "                    \"Night Mins\": 0.04217172791151364,\n",
      "                    \"Night Calls\": 0.10668551884258355,\n",
      "                    \"Intl Mins\": 0.01769624023473138,\n",
      "                    \"Intl Calls\": 0.032804043212300085,\n",
      "                    \"CustServ Calls\": 0.06748383327380432,\n",
      "                    \"State_AK\": 0.015546370841387084,\n",
      "                    \"State_AL\": 0.015435590446799707,\n",
      "                    \"State_AR\": 0.0162048124423131,\n",
      "                    \"State_AZ\": 0.016174332970606324,\n",
      "                    \"State_CA\": 0.016234480948887946,\n",
      "                    \"State_CO\": 0.016051134340711214,\n",
      "                    \"State_CT\": 0.016560579646848296,\n",
      "                    \"State_DC\": 0.01605482430837602,\n",
      "                    \"State_DE\": 0.01611204121235586,\n",
      "                    \"State_FL\": 0.015274766574893782,\n",
      "                    \"State_GA\": 0.015920066945834893,\n",
      "                    \"State_HI\": 0.015215403486200632,\n",
      "                    \"State_IA\": 0.016744496505783673,\n",
      "                    \"State_ID\": 0.01563949778051464,\n",
      "                    \"State_IL\": 0.015302583016802,\n",
      "                    \"State_IN\": 0.016466992694190725,\n",
      "                    \"State_KS\": 0.016238277573982756,\n",
      "                    \"State_KY\": 0.016528426372325405,\n",
      "                    \"State_LA\": 0.01556326753316345,\n",
      "                    \"State_MA\": 0.015386919545003578,\n",
      "                    \"State_MD\": 0.01514914727925056,\n",
      "                    \"State_ME\": 0.015847814879204457,\n",
      "                    \"State_MI\": 0.01585896127697173,\n",
      "                    \"State_MN\": 0.015152028097658157,\n",
      "                    \"State_MO\": 0.015135025449423335,\n",
      "                    \"State_MS\": 0.015995644972863404,\n",
      "                    \"State_MT\": 0.015860061822872626,\n",
      "                    \"State_NC\": 0.015873089571483832,\n",
      "                    \"State_ND\": 0.016725875062164507,\n",
      "                    \"State_NE\": 0.01665292631116723,\n",
      "                    \"State_NH\": 0.015943231909630907,\n",
      "                    \"State_NJ\": 0.01566928260940535,\n",
      "                    \"State_NM\": 0.015120419157086477,\n",
      "                    \"State_NV\": 0.015436094402573609,\n",
      "                    \"State_NY\": 0.015889923873070415,\n",
      "                    \"State_OH\": 0.01519542847466281,\n",
      "                    \"State_OK\": 0.01678276851172869,\n",
      "                    \"State_OR\": 0.01520429288762556,\n",
      "                    \"State_PA\": 0.01707469872457348,\n",
      "                    \"State_RI\": 0.01591550577999367,\n",
      "                    \"State_SC\": 0.016412525751184154,\n",
      "                    \"State_SD\": 0.01585812603577606,\n",
      "                    \"State_TN\": 0.015292336718357039,\n",
      "                    \"State_TX\": 0.016271836534964047,\n",
      "                    \"State_UT\": 0.01555480229246551,\n",
      "                    \"State_VA\": 0.015640491684769896,\n",
      "                    \"State_VT\": 0.01580520094661424,\n",
      "                    \"State_WA\": 0.01585987381320435,\n",
      "                    \"State_WI\": 0.01585738853789069,\n",
      "                    \"State_WV\": 0.016206974780204577,\n",
      "                    \"State_WY\": 0.01655834109645995,\n",
      "                    \"Area Code_657\": 0.015736304983229688,\n",
      "                    \"Area Code_658\": 0.015397095345794927,\n",
      "                    \"Area Code_659\": 0.015395155579983602,\n",
      "                    \"Area Code_676\": 0.01583686873981163,\n",
      "                    \"Area Code_677\": 0.016244678298108835,\n",
      "                    \"Area Code_678\": 0.016177563131541256,\n",
      "                    \"Area Code_686\": 0.01596267401330929,\n",
      "                    \"Area Code_707\": 0.01625121070255585,\n",
      "                    \"Area Code_716\": 0.016938215694187037,\n",
      "                    \"Area Code_727\": 0.01582774122470359,\n",
      "                    \"Area Code_736\": 0.015421742108170747,\n",
      "                    \"Area Code_737\": 0.015748860729473263,\n",
      "                    \"Area Code_758\": 0.01575104283495403,\n",
      "                    \"Area Code_766\": 0.0159125886729695,\n",
      "                    \"Area Code_776\": 0.015879199513906653,\n",
      "                    \"Area Code_777\": 0.015860712247402497,\n",
      "                    \"Area Code_778\": 0.015062008667690982,\n",
      "                    \"Area Code_786\": 0.015973427255641097,\n",
      "                    \"Area Code_787\": 0.01603752304542287,\n",
      "                    \"Area Code_788\": 0.016216800561033962,\n",
      "                    \"Area Code_797\": 0.0158386883690628,\n",
      "                    \"Area Code_798\": 0.016006535073622194,\n",
      "                    \"Area Code_806\": 0.015432544045279727,\n",
      "                    \"Area Code_827\": 0.016581437845493492,\n",
      "                    \"Area Code_836\": 0.01610872677576455,\n",
      "                    \"Area Code_847\": 0.016210476674381906,\n",
      "                    \"Area Code_848\": 0.016107500204571273,\n",
      "                    \"Area Code_858\": 0.015815151724795796,\n",
      "                    \"Area Code_866\": 0.015226173679064973,\n",
      "                    \"Area Code_868\": 0.015378382964927944,\n",
      "                    \"Area Code_876\": 0.016354344536795185,\n",
      "                    \"Area Code_877\": 0.01670992045700297,\n",
      "                    \"Area Code_878\": 0.01570883831239962,\n",
      "                    \"Int'l Plan_no\": 0.016152742977145494,\n",
      "                    \"Int'l Plan_yes\": 0.015806001626031466,\n",
      "                    \"VMail Plan_no\": 0.02576006106750696,\n",
      "                    \"VMail Plan_yes\": 0.03847279835958336\n",
      "                },\n",
      "                \"expected_value\": 0.7418107986450195\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model_explainability_monitor.latest_baselining_job.wait(logs=False)\n",
    "model_explainability_constraints = model_explainability_monitor.suggested_constraints()\n",
    "print()\n",
    "print(\n",
    "    f\"ModelExplainabilityMonitor suggested constraints: {model_explainability_constraints.file_s3_uri}\"\n",
    ")\n",
    "print(S3Downloader.read_file(model_explainability_constraints.file_s3_uri, sagemaker_session=sagemaker_session))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedule model explainability monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `create_monitoring_schedule()` method to schedule a hourly monitor, to analyze the data with monitoring schedule. If a baselining job has been submitted, then the monitor will automatically pick up analysis configuration from the baselining job. But if the baselining step is skipped, or the capture dataset has different nature than the training dataset, then analysis configuration has to be provided.\n",
    "\n",
    "`ModelConfig` is required by `ExplainabilityAnalysisConfig` for the same reason as it is required by the baselining job. Note that only features are required for computing feature attribution, so ground truth label should be excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_explainability_analysis_config = None\n",
    "if not model_explainability_monitor.latest_baselining_job:\n",
    "    # Remove label because only features are required for the analysis\n",
    "    headers_without_label_header = copy.deepcopy(all_headers)\n",
    "    headers_without_label_header.remove(label_header)\n",
    "    model_explainability_analysis_config = ExplainabilityAnalysisConfig(\n",
    "        explainability_config=shap_config,\n",
    "        model_config=model_config,\n",
    "        headers=headers_without_label_header,\n",
    "    )\n",
    "model_explainability_monitor.create_monitoring_schedule(\n",
    "    output_s3_uri=s3_report_path,\n",
    "    endpoint_input=endpoint_name,\n",
    "    schedule_cron_expression=schedule_expression,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for execution and inspect analysis results\n",
    "\n",
    "Once created the schedule is started by default, here wait for the its first execution to start, then stop the schedule to avoid incurring charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A hourly schedule was created above and it will kick off executions ON the hour (plus 0 - 20 min buffer).\n",
      "Waiting for the first execution to happen......................Uploading 500 records to s3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/ground_truth_data/2021-10-20-08-11-20/2021/10/20/15/2917.jsonl\n",
      "......................................\n",
      "Done! Execution has been created\n",
      "Now waiting for execution to start....\n",
      "Done! Execution has started\n"
     ]
    }
   ],
   "source": [
    "wait_for_execution_to_start(model_explainability_monitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping Monitoring Schedule with name: monitoring-schedule-2021-10-20-15-07-00-878\n"
     ]
    }
   ],
   "source": [
    "model_explainability_monitor.stop_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait further for the execution to finish, then inspect its analysis results,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for execution to finish.............\n",
      "Done! Execution has finished\n",
      "Uploading 500 records to s3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/ground_truth_data/2021-10-20-08-11-20/2021/10/20/16/2918.jsonl\n"
     ]
    }
   ],
   "source": [
    "wait_for_execution_to_finish(model_explainability_monitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report URI: s3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/reports/DEMO-xgb-churn-model-monitor-2021-10-20-1311/monitoring-schedule-2021-10-20-15-07-00-878/2021/10/20/16\n",
      "Found Report Files:\n",
      "s3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/reports/DEMO-xgb-churn-model-monitor-2021-10-20-1311/monitoring-schedule-2021-10-20-15-07-00-878/2021/10/20/16/analysis.json\n",
      " s3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/reports/DEMO-xgb-churn-model-monitor-2021-10-20-1311/monitoring-schedule-2021-10-20-15-07-00-878/2021/10/20/16/report.html\n",
      " s3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/reports/DEMO-xgb-churn-model-monitor-2021-10-20-1311/monitoring-schedule-2021-10-20-15-07-00-878/2021/10/20/16/report.ipynb\n",
      " s3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/reports/DEMO-xgb-churn-model-monitor-2021-10-20-1311/monitoring-schedule-2021-10-20-15-07-00-878/2021/10/20/16/report.pdf\n"
     ]
    }
   ],
   "source": [
    "schedule_desc = model_explainability_monitor.describe_schedule()\n",
    "execution_summary = schedule_desc.get(\"LastMonitoringExecutionSummary\")\n",
    "if execution_summary and execution_summary[\"MonitoringExecutionStatus\"] in [\n",
    "    \"Completed\",\n",
    "    \"CompletedWithViolations\",\n",
    "]:\n",
    "    last_model_explainability_monitor_execution = model_explainability_monitor.list_executions()[-1]\n",
    "    last_model_explainability_monitor_execution_report_uri = (\n",
    "        last_model_explainability_monitor_execution.output.destination\n",
    "    )\n",
    "    print(f\"Report URI: {last_model_explainability_monitor_execution_report_uri}\")\n",
    "    last_model_explainability_monitor_execution_report_files = sorted(\n",
    "        S3Downloader.list(last_model_explainability_monitor_execution_report_uri, sagemaker_session=sagemaker_session)\n",
    "    )\n",
    "    print(\"Found Report Files:\")\n",
    "    print(\"\\n \".join(last_model_explainability_monitor_execution_report_files))\n",
    "else:\n",
    "    last_model_explainability_monitor_execution = None\n",
    "    print(\n",
    "        \"====STOP==== \\n No completed executions to inspect further. Please wait till an execution completes or investigate previously reported failures.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are any violations compared to the baseline, they will be listed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Could not retrieve constraints file at location 's3://sagemaker-us-east-1-377983720232/sagemaker/DEMO-ClarifyModelMonitor-20200901/reports/DEMO-xgb-churn-model-monitor-2021-10-20-1311/monitoring-schedule-2021-10-20-15-07-00-878/2021/10/20/16/constraint_violations.json'. To manually retrieve ConstraintViolations object from a given uri, use 'my_model_monitor.constraints(my_s3_uri)' or 'ConstraintViolations.from_s3_uri(my_s3_uri)'\n"
     ]
    }
   ],
   "source": [
    "if last_model_explainability_monitor_execution:\n",
    "    model_explainability_violations = (\n",
    "        last_model_explainability_monitor_execution.constraint_violations()\n",
    "    )\n",
    "    if model_explainability_violations:\n",
    "        print(model_explainability_violations.body_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis results and CloudWatch metrics are visualized in SageMaker Studio. Select the Endpoints tab, then double click the endpoint to show the UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART D: Cleanup\n",
    "\n",
    "The endpoint can keep running and capturing data, but if there is no plan to collect more data or use this endpoint further, it should be deleted to avoid incurring additional charges. Note that deleting endpoint does not delete the data that was captured during the model invocations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First stop the worker threads,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoke_endpoint_thread.terminate()\n",
    "ground_truth_thread.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then stop all monitors scheduled for the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping Monitoring Schedule with name: monitoring-schedule-2021-10-20-15-07-00-878\n",
      "Waiting for execution to finish\n",
      "Done! Execution has finished\n",
      "\n",
      "Deleting Monitoring Schedule with name: monitoring-schedule-2021-10-20-15-07-00-878\n",
      "\n",
      "Stopping Monitoring Schedule with name: monitoring-schedule-2021-10-20-13-32-24-036\n",
      "Waiting for execution to finish\n",
      "Done! Execution has finished\n",
      "\n",
      "Deleting Monitoring Schedule with name: monitoring-schedule-2021-10-20-13-32-24-036\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "predictor = Predictor(endpoint_name, sagemaker_session=sagemaker_session)\n",
    "model_monitors = predictor.list_monitors()\n",
    "for model_monitor in model_monitors:\n",
    "    model_monitor.stop_monitoring_schedule()\n",
    "    wait_for_execution_to_finish(model_monitor)\n",
    "    model_monitor.delete_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()\n",
    "predictor.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
